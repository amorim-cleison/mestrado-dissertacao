\section{Preparação dos modelos}
\label{sec:metodos-preparacao-modelos}

% SELEÇÃO DOS MODELOS -------------------------------------------

Tomando como referência a discussão introduzida na \autoref{sec:am-ap}, serão adotados nos experimentos deste trabalho três das principais arquiteturas utilizadas em tarefas de \acrfull{nlp}: o \textit{Encoder-Decoder} em uma versão com \acrfull{lstm} e outra com \acrfull{gru}, e também o \textit{Transformer}.

Para estabelecer os parâmetros dessas arquiteturas, as estratégias de otimização e de treinamento, bem como as métricas utilizadas nos experimentos, foram consideradas as discussões apresentadas por \citeonline{goodfellow-2016-deep-learning} e pela \autoref{sec:am}.

Dessa forma, o algoritmo de otimização dos modelos será definido como o \acrfull{sgd} com \textit{momentum} de 0,9 \cite{robbins-2007-stochastic}. Ele será combinado a uma estratégia de redução da taxa de aprendizagem por um fator de 0,2 sempre que o valor do erro médio calculado atingir um platô por 5 épocas seguidas.

A função objetivo (ou função de perda), por sua vez, será a \acrfull{cel} \cite{mitchell-1997-ml}, que é apresentada na \autoref{eqn:cross-entropy-loss}. Nela, \(p\) representa as probabilidades ou pontuações estimadas pelo modelo para as amostras e \(y\) corresponde ao valor correto esperado para essas estimativas:

\begin{equation}
    \label{eqn:cross-entropy-loss}
    L_{\log}(y, p) = -(y \log (p) + (1 - y) \log (1 - p))
\end{equation}


Os dados serão particionados numa proporção de 15\% para o subconjunto de validação, 15\% para o de testes e o restante para o subconjunto treinamento. Os \textit{batches} (ou lotes), por sua vez, possuirão tamanho de 50 amostras.



A seleção dos hiperparâmetros dos modelos foi realizada utilizando-se o algoritmo \textit{Grid Search} (ou busca em grade) com validação cruzada de 5 \textit{folds}. O conjunto de valores de hiperparâmetros utilizados na busca estão apresentados na \autoref{tab:otim-params} e as combinações que melhor reduziram o erro médio para cada modelo foram as seguintes:

\input{capitulos/metodos/tabelas/otim_params}

\begin{itemize}
    \item \textit{Encoder-Decoder} com \acrshort{lstm}: taxa de aprendizagem de 0,1; \textit{dropout} de 0,1; \textit{embeddings} com dimensão de 1024; camadas ocultas com dimensão de 512; e utilização de 2 camadas de \acrshort{lstm} no \textit{encoder} e no \textit{decoder}.

    \item \textit{Encoder-Decoder} com \acrshort{gru}: taxa de aprendizagem de 0,01; \textit{dropout} de 0,1; \textit{embeddings} com dimensão de 1024; camadas ocultas com dimensão de 512; e utilização de 2 camadas de \acrshort{gru} no \textit{encoder} e no \textit{decoder}.

    \item \textit{Transformer}: taxa de aprendizagem de 0,1; \textit{dropout} de 0,5; \textit{embeddings} com dimensão de 512; camadas ocultas com dimensão de 512; utilização de 2 camadas e de 8 cabeças de \textit{attention}.
\end{itemize}



O código-fonte utilizado nos experimentos deste trabalho foi disponibilizado através do endereço indicado abaixo\footnote{
    Disponível em \url{https://www.cin.ufpe.br/~cca5/sl-nlp}.
}.
