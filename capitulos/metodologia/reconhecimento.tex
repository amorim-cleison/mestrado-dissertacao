\section{Reconhecendo os sinais}
\label{sec:metodologia-reconhecimento}

Uma vez que temos um \textit{dataset} com atributos fonológicos processados, agora podemos prosseguir com a preparação das features e dos modelos para reconhecimentos dos sinais utilizando a abordagem proposta.

Para tornar as amostras do ASL-Phono compatíveis com a entrada dos modelos que serão utilizados mais adiante, aplicamos uma transformação simples que consiste em codificar seus atributos fonológicos como blocos ou ``palavras'' únicas, mais compactas. Ou seja, ao invés de utilizar como entrada sequências de frames que contêm propriedades aninhadas, adotamos aqui sequências de blocos compactos que codificam a mesma informação de uma forma mais amigável a modelos sequenciais.

Observe na \autoref{tab:codificacao-bloco} um exemplo deste processo. Na primeira linha, temos os valores originais das propriedades providas para o frame de uma amostra do ASL-Phono; na segunda linha, geram-se acrônimos a partir desses atributos para realizar uma primeira compactação -- mas que não é um passo necessariamente obrigatório; na terceira linha, temos um bloco ou palavra única gerada pela composição desses acrônimos; finalmente, na última linha, temos o exemplo de uma sequência desses blocos.

\input{capitulos/metodologia/tabelas/codificacao_bloco}


% RESAMPLING DO DATASET -------------------------------------------

Conforme observa-se pela \autoref{tab:dataset-phono-stats}, o ASL-Phono apresenta um desbalanceamento no número de amostras disponíveis por sinal. Em média, há 3,68 amostras disponíveis por sinal porém, enquanto alguns sinais têm apenas 1 amostra, outros apresentam um número atípico de 59. Isso poderia influenciar o desempenho dos modelos utilizados adiante, uma vez que durante o treinamento alguns sinais seriam favorecidos e, outros, penalizados. 

Por conta disso, aplicamos dois procedimentos para reduzir o desbalanceamento desses dados: primeiro, eliminamos os sinais com apenas 1 amostra, uma vez que esse número é insuficiente para que o modelo seja capaz de aprender e generalizar acerca desses sinais, sobretudo quando o \textit{dataset} é particionado durante o treinamento; segundo, realizamos uma reamostragem do \textit{dataset} para equilibrar o número de amostras.

Observe na \autoref{subfig:dataset-resampling-antes} um perfil detalhado dos dados antes da reamostragem. É possível perceber que o número de amostras é bastante disperso pelo eixo horizontal do gráfico: por exemplo, 726 sinais apresentam 2 amostras disponíveis; seguidos por 552 sinais com 4 amostras; mas, no outro extremo, temos que apenas um número muito pequeno de sinais (entre 1 e 3) apresentam um grande número de amostras disponíveis (entre 13 e 59), respectivamente.

\begin{figure}[ht!]
    \centering
    \caption{
        \textmd{Contagem de número de amostras por sinal (\subref{subfig:dataset-resampling-antes}) no \textit{dataset} original e (\subref{subfig:dataset-resampling-depois}) no \textit{dataset} balanceado, após re-amostragem dos dados.}
    }
    \subcaptionbox{\label{subfig:dataset-resampling-antes}}{
        \includegraphics[height=6cm]{capitulos/metodologia/imagens/dataset_resampling_antes}
    }%
    \hspace{1cm}
    \subcaptionbox{\label{subfig:dataset-resampling-depois}}{
        \includegraphics[height=6cm]{capitulos/metodologia/imagens/dataset_resampling_depois}
    }%
    \nomefonte{}
    \label{fig:dataset-resampling}
\end{figure}

Aplicamos então uma reamostragem \textit{naive random under-sampling} (ou sub-amostragem aleatória ingênua, que reduz o número de amostras através de uma seleção aleatória de algumas delas para os sinais que estão super-representados) seguida de uma \textit{naive random over-sampling} (ou sobre-amostragem aleatória ingênua, que replica aleatoriamente algumas das amostras existentes para os sinais que estão sub-representados).
Para estabelecer o novo número de amostras por sinal \(x'\), utilizamos a \autoref{eqn:resampling-target}: 

\begin{equation}
    \label{eqn:resampling-target}
    x' = round( \overline{a} + \ln(x) )
\end{equation}

Onde \(\overline{a}\) refere-se ao número médio de amostras por sinal no \textit{dataset} e \(x\) é o número de amostras para aquele sinal. Observe na \autoref{subfig:dataset-resampling-depois} como os dados ficaram organizados após a reamostragem. Em resumo, a equação faz com que o número de amostras por sinal se distribua mais uniformemente em torno da média.



% SELEÇÃO DOS MODELOS -------------------------------------------

% Com relação aos modelos utilizados para avaliar a eficácia da abordagem proposta ao reconhecer sinais, selecionamos três arquiteturas clássicas de modelos sequenciais -- que são LSTM \cite{hochreiter-1997-lstm}, GRU \cite{cho-2014-gru} e Transformer \cite{vaswani-2017-transformer} --, as quais nos ajudarão a estabelecer uma linha de base de referência. A partir do desempenho aferidos para essas arquiteturas, objetiva-se também prover clareza acerca de qual delas lida aborda melhor o problema em questão e é mais promissora para novas pesquisas nessa mesma linha.

Segundo \citeonline{jurafsky-2022-speech-lang-processing}, a linguagem é um fenômeno inerentemente temporal, e pode ser compreendida como uma sequência de eventos que se desdobram ao longo do tempo, como um fluxo contínuo de dados.
Por esse motivo, para avaliar a eficácia da abordagem proposta, selecionamos três arquiteturas clássicas de aprendizagem profunda de processamento de sequências, as quais são: \acrfull{lstm}~\cite{hochreiter-1997-lstm} e \acrfull{gru}~\cite{cho-2014-gru} -- que são extensões da arquitetura \acrfull{rnn}~\cite{mikolov-2010-rnn} -- e o Transformer~\cite{vaswani-2017-transformer}. Essas arquiteturas possuem mecanismos que lhes permitem capturar e explorar a natureza temporal da linguagem. 

\acrshortpl{rnn} são redes que contém um ciclo (ou recorrência) em suas conexões, significando que o valor de uma unidade é direta ou indiretamente dependente de seu próprias saídas anteriores, afirmam \citeonline{jurafsky-2022-speech-lang-processing}. 
Elas processam a sequência de entrada uma palavra por vez, tentando prever a próxima palavra com base na palavra atual e no estado oculto anterior. Essas redes não sofrem do problema do contexto limitado, uma vez que o estado oculto pode, em princípio, representar informações sobre todas as palavras anteriores desde o início da sequência. No entanto, essas redes podem ser difíceis de racionar a respeito ou de treinar. 

\citeonline{lecun-2015-deep-learning} afirmam que apesar do principal objetivo das \acrshortpl{rnn} ser aprender dependências de longo prazo, evidências teóricas e empíricas mostram que há desafios em aprender a armazenar informações por um tempo muito longo. Entre esses desafios estão os problemas de desaparecimento e explosão de gradientes, aos quais as \acrshortpl{rnn} estão passíveis. Devido a isso, várias extensões dessa arquitetura foram desenvolvidas no decorrer dos anos com o intuito de abordar melhor esses problemas.

O \acrshort{lstm} é a mais popular dentre elas. Ele divide o gerenciamento do contexto em duas partes: na primeira, está a adição de informação que provavelmente será necessária para tomada de decisão posterior ao contexto; na segunda, está a remoção de informação que não é mais necessária. Com isso, o \acrshort{lstm} é capaz de aprender como gerenciar esse contexto e lidar com ambas as partes, e não exige que uma estratégia para isso seja codificada na arquitetura.
Isso é feito utilizando-se uma camada explícita para representar o contexto e também unidades neurais especializadas que utilizam três portas (\textit{update gate}, \textit{forget gate} e \textit{output gate}) para controlar o fluxo de informações para dentro e para fora das unidades que compõem as camadas da rede neural. Essas portas, por sua vez, são implementadas como pesos adicionais que são ajustados durante o processo de treinamento e operam sequencialmente na entrada, na camada oculta e nas camadas de contexto anteriores \cite{jurafsky-2022-speech-lang-processing}.

\citeonline{goodfellow-2016-deep-learning,lecun-2015-deep-learning} ressaltam que as redes \acrshort{lstm} mostraram ser extremamente bem-sucedidas em diferentes tipos de aplicações, como reconhecimento de caligrafia, reconhecimento de fala, geração de caligrafia, tradução automática, legendagem de imagens e análise sintática.

A arquitetura \acrshort{gru} surgiu com o objetivo de simplificar o desenho das unidades internas do \acrshort{lstm} e, devido a isso, é considerada uma evolução desta. Segundo \citeonline{goodfellow-2016-deep-learning}, a principal diferença entre elas está na forma como elas controlam o fluxo de informação entre suas camadas. Enquanto que nas unidades do \acrshort{lstm} são utilizadas três portas para controlar a atualização, esquecimento e a saída de informação, no \acrshort{gru} uma única porta realiza o controle simultâneo do fator de esquecimento e da atualização do estado da unidade -- o que faz com que ela apresentem um total duas portas (denominadas \textit{update gate} e \textit{reset gate}) \cite{ravanelli-2018-li-gru,goodfellow-2016-deep-learning}.

% TODO: falar de encoder-decoder?


O Transformer, por sua vez, é uma arquitetura que não utiliza recorrência (como acontece nas \acrshortpl{rnn}) e, ao invés disso, baseia-se inteiramente em um mecanismo de \textit{attention} (ou atenção) para estabelecer dependências globais entre os dados de entrada e saída \cite{vaswani-2017-transformer}. 
Essa arquitetura é composta por blocos empilhados, que consistem de redes multicamadas contendo camadas lineares simples, redes \textit{feed-forward} e camadas de \textit{self-attention} (ou auto-atenção) -- que é a principal inovação dos Transformers. O \textit{self-attention} permite que uma rede neural extraia e utilize diretamente informações de contextos arbitrariamente grandes sem a necessidade de transmiti-los por meio de conexões recorrentes intermediárias como nas \acrshortpl{rnn} \cite{jurafsky-2022-speech-lang-processing}.

Segundo \citeonline{wolf-2020-transformers}, o Transformer tornou-se rapidamente a arquitetura dominante para o \acrshort{nlp}, e tem superado modelos alternativos como \acrshortpl{cnn} e \acrshortpl{rnn} no desempenho de tarefas de compreensão e geração de linguagem natural. Sua arquitetura é escalável de acordo com os dados de treinamento e o tamanho do modelo, e facilita o treinamento paralelo eficiente e a captura do contexto de sequências de longo alcance \cite{wolf-2020-transformers}.




% This chapter introduces two important deep learning architectures designed to address these challenges: recurrent neural networks and transformer networks. Both approaches have mechanisms to deal directly with the sequential nature of language that allow them to capture and exploit the temporal nature of language. The recurrent network offers a new way to represent the prior context, allowing the model’s decision to depend on information from hundreds of words in the past. The transformer offers new mechanisms (self-attention and positional encodings) that help represent time and help focus on how words relate to each other over long distances.


% LSTM
% LSTM networks have been shown to learn long-term dependencies more easilythan the simple recurrent architectures, ﬁrst on artiﬁcial datasets designed fortesting the ability to learn long-term dependencies (Bengio et al., 1994; Hochreiterand Schmidhuber, 1997; Hochreiter et al., 2001), then on challenging sequenceprocessing tasks where state-of-the-art performance was obtained (Graves, 2012;Graves et al., 2013; Sutskever et al., 2014). Variants and alternatives to the LSTMthat have been studied and used are discussed next.
% The LSTM has been found extremely successfulin many applications, such as unconstrained handwriting recognition (Graveset al., 2009), speech recognition (Graves et al., 2013; Graves and Jaitly, 2014),handwriting generation (Graves, 2013), machine translation (Sutskever et al., 2014),image captioning (Kiros et al., 2014b; Vinyals et al., 2014b; Xu et al., 2015), andparsing (Vinyals et al., 2014a).
% \cite{goodfellow-2016-deep-learning}


% GRU
% Which pieces of the LSTM architecture are actually necessary? What other successful architectures could be designed that allow the network to dynamicallycontrol the time scale and forgetting behavior of diﬀerent units? Some answers to these questions are given with the recent work on gated RNNs, whose units are also known as gated recurrent units, or GRUs (Cho et al., 2014b;Chung et al., 2014, 2015a; Jozefowicz et al., 2015; Chrupala et al., 2015).
% \cite{goodfellow-2016-deep-learning}

% The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit.
% \cite{goodfellow-2016-deep-learning}

% This evolution has recently led to a novel architecture called Gated Recurrent Unit (GRU) [8], that simplifies the complex LSTM cell design.
% [...]
% A noteworthy attempt to simplify LSTMs has recently led to a novel model called Gated Recurrent Unit (GRU) [8], [47], that is based on just two multiplicative gates.
% \cite{ravanelli-2018-li-gru}


% TRANSFORMER
% \cite{jurafsky-2022-speech-lang-processing}
% transformers – an approach to sequence processing that eliminates recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in Chapter 7.
% Transformers map sequences of input vectors (x1; :::;xn) to sequences of output vectors (y1; :::;yn) of the same length. 
% Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, the key innovation of transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We’ll start by describing how self-attention works and then return to how it fits into larger transformer blocks.


% \cite{vaswani-2017-transformer}
% In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.


% \cite{wolf-2020-transformers}
% Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks.

% The Transformer (Vaswani et al., 2017) has rapidly become the dominant architecture for natural language processing, surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation. The architecture scales with training data and model size, facilitates efficient parallel training, and captures long-range sequence features


% Experimento
%   Preparação das features (asl-phono -> palavras)
%       Transformação das sequências no dataset: frames -> palavras
%       Justificativa??
%   Preparação dos modelos (Transformer, LSTM, GRU, etc) -- por modelo
%       Arquitetura + Parâmetros
%       lr scheduler, optimizer, loss function
%       Busca de parâmetros (dimensionar os modelos/parâmetros)






% \cite{jurafsky-2022-speech-lang-processing}
% The most commonly used such extension to RNNs is the Long short-term
% memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two sub-problems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers.
% The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased.



% ======================================
% \cite{goodfellow-2016-deep-learning}
% 
% Chapter 10
% https://www.deeplearningbook.org/contents/rnn.html
%
% 10.10 The Long Short-Term Memory and Other GatedRNNs (pg 404)
% As of this writing, the most effective sequence models used in practical applications are called gated RNNs. These include the long short-term memory and networks based on the gated recurrent unit.
% Like leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did this with connection weights that were either manually chosen constants or were parameters. Gated RNNs generalize this to connection weights that may change at each time step.
% Leaky units allow the network to accumulate information (such as evidence fora particular feature or category) over a long duration. Once that information has been used, however, it might be useful for the neural network to forget the old state. For example, if a sequence is made of subsequences and we want a leaky unit to accumulate evidence inside each sub-subsequence, we need a mechanism to forget the old state by setting it to zero. Instead of manually deciding when to clear the state, we want the neural network to learn to decide when to do it. This is what gated RNNs do.

% 10.10.1 LSTM
% The clever idea of introducing self-loops to produce paths where the gradientcan ﬂow for long durations is a core contribution of the initiallong short-termmemory(LSTM) model (Hochreiter and Schmidhuber, 1997). A crucial additionhas been to make the weight on this self-loop conditioned on the context, rather thanﬁxed (Gers et al., 2000). By making the weight of this self-loop gated (controlledby another hidden unit), the time scale of integration can be changed dynamically.In this case, we mean that even for an LSTM with ﬁxed parameters, the time scaleof integration can change based on the input sequence, because the time constantsare output by the model itself. 
% The LSTM has been found extremely successfulin many applications, such as unconstrained handwriting recognition (Graveset al., 2009), speech recognition (Graves et al., 2013; Graves and Jaitly, 2014),handwriting generation (Graves, 2013), machine translation (Sutskever et al., 2014),image captioning (Kiros et al., 2014b; Vinyals et al., 2014b; Xu et al., 2015), andparsing (Vinyals et al., 2014a).
% [...]
% Deeper architectures have also been successfully used (Graves et al.,2013; Pascanu et al., 2014a). Instead of a unit that simply applies an element-wisenonlinearity to the aﬃne transformation of inputs and recurrent units, LSTMrecurrent networks have “LSTM cells” that have an internal recurrence (a self-loop),in addition to the outer recurrence of the RNN. Each cell has the same inputs andoutputs as an ordinary recurrent network, but also has more parameters and asystem of gating units that controls the ﬂow of information.
% LSTM networks have been shown to learn long-term dependencies more easilythan the simple recurrent architectures, ﬁrst on artiﬁcial datasets designed fortesting the ability to learn long-term dependencies (Bengio et al., 1994; Hochreiterand Schmidhuber, 1997; Hochreiter et al., 2001), then on challenging sequenceprocessing tasks where state-of-the-art performance was obtained (Graves, 2012;Graves et al., 2013; Sutskever et al., 2014). Variants and alternatives to the LSTMthat have been studied and used are discussed next.

% 10.10.2 Other Gated RNNs
% Which pieces of the LSTM architecture are actually necessary? What other successful architectures could be designed that allow the network to dynamically control the time scale and forgetting behavior of different units? Some answers to these questions are given with the recent work on gated RNNs,whose units are also known as gated recurrent units, or GRUs (Cho et al., 2014b;Chung et al., 2014, 2015a; Jozefowicz et al., 2015; Chrupala et al., 2015). The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit.



% ================================
% \cite{lecun-2015-deep-learning}
% RNNs, once unfolded in time (Fig. 5), can be seen as very deep feedforward networks in which all the layers share the same weights. Although their main purpose is to learn long-term dependencies, theoretical and empirical evidence shows that it is difficult to learn to store information for very long78.
% To correct for that, one idea is to augment the network with an explicit memory. The first proposal of this kind is the long short-term memory (LSTM) networks that use special hidden units, the natural behaviour of which is to remember inputs for a long time79. A special unit called the memory cell acts like an accumulator or a gated leaky neuron: it has a connection to itself at the next time step that has a weight of one, so it copies its own real-valued state and accumulates the external signal, but this self-connection is multiplicatively gated by another unit that learns to decide when to clear the content of the memory.
% LSTM networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76
