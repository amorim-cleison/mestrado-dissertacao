\section{Reconhecendo os sinais}
\label{sec:metodologia-reconhecimento}

Uma vez que temos um \textit{dataset} com atributos fonológicos processados, agora podemos prosseguir com a preparação das features e dos modelos para reconhecimentos dos sinais utilizando a abordagem proposta.

Para tornar as amostras do ASL-Phono compatíveis com a entrada dos modelos que serão utilizados mais adiante, aplicamos uma transformação simples que consiste em codificar seus atributos fonológicos como blocos ou ``palavras'' únicas, mais compactas. Ou seja, ao invés de utilizar como entrada sequências de frames que contêm propriedades aninhadas, adotamos aqui sequências de blocos compactos que codificam a mesma informação de uma forma mais amigável a modelos sequenciais.

Observe na \autoref{tab:codificacao-bloco} um exemplo deste processo. Na primeira linha, temos os valores originais das propriedades providas para o frame de uma amostra do ASL-Phono; na segunda linha, geram-se acrônimos a partir desses atributos para realizar uma primeira compactação -- mas que não é um passo necessariamente obrigatório; na terceira linha, temos um bloco ou palavra única gerada pela composição desses acrônimos; finalmente, na última linha, temos o exemplo de uma sequência desses blocos.

\input{capitulos/metodologia/tabelas/codificacao_bloco}


% TODO: falar do re-sampling do dataset


Com relação aos modelos utilizados para avaliar a eficácia da abordagem proposta ao reconhecer sinais, selecionamos três arquiteturas clássicas de modelos sequenciais -- que são LSTM \cite{hochreiter-1997-lstm}, GRU \cite{cho-2014-gru} e Transformer \cite{vaswani-2017-transformer} --, as quais nos ajudarão a estabelecer uma linha de base de referência. A partir do desempenho aferidos para essas arquiteturas, objetiva-se também prover clareza acerca de qual delas lida aborda melhor o problema em questão e é mais promissora para novas pesquisas nessa mesma linha.
 





% Experimento
%   Preparação das features (asl-phono -> palavras)
%       Transformação das sequências no dataset: frames -> palavras
%       Justificativa??
%   Preparação dos modelos (Transformer, LSTM, GRU, etc) -- por modelo
%       Arquitetura + Parâmetros
%       lr scheduler, optimizer, loss function
%       Busca de parâmetros (dimensionar os modelos/parâmetros)



% ======================================
% \cite{goodfellow-2016-deep-learning}
% 
% Chapter 10
% https://www.deeplearningbook.org/contents/rnn.html
%
% 10.10 The Long Short-Term Memory and Other GatedRNNs (pg 404)
% As of this writing, the most effective sequence models used in practical applications are called gated RNNs. These include the long short-term memory and networks based on the gated recurrent unit.
% Like leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did this with connection weights that were either manually chosen constants or were parameters. Gated RNNs generalize this to connection weights that may change at each time step.
% Leaky units allow the network to accumulate information (such as evidence fora particular feature or category) over a long duration. Once that information has been used, however, it might be useful for the neural network to forget the old state. For example, if a sequence is made of subsequences and we want a leaky unit to accumulate evidence inside each sub-subsequence, we need a mechanism to forget the old state by setting it to zero. Instead of manually deciding when to clear the state, we want the neural network to learn to decide when to do it. This is what gated RNNs do.

% 10.10.1 LSTM
% The clever idea of introducing self-loops to produce paths where the gradientcan ﬂow for long durations is a core contribution of the initiallong short-termmemory(LSTM) model (Hochreiter and Schmidhuber, 1997). A crucial additionhas been to make the weight on this self-loop conditioned on the context, rather thanﬁxed (Gers et al., 2000). By making the weight of this self-loop gated (controlledby another hidden unit), the time scale of integration can be changed dynamically.In this case, we mean that even for an LSTM with ﬁxed parameters, the time scaleof integration can change based on the input sequence, because the time constantsare output by the model itself. The LSTM has been found extremely successfulin many applications, such as unconstrained handwriting recognition (Graveset al., 2009), speech recognition (Graves et al., 2013; Graves and Jaitly, 2014),handwriting generation (Graves, 2013), machine translation (Sutskever et al., 2014),image captioning (Kiros et al., 2014b; Vinyals et al., 2014b; Xu et al., 2015), andparsing (Vinyals et al., 2014a).
% [...]
% Deeper architectures have also been successfully used (Graves et al.,2013; Pascanu et al., 2014a). Instead of a unit that simply applies an element-wisenonlinearity to the aﬃne transformation of inputs and recurrent units, LSTMrecurrent networks have “LSTM cells” that have an internal recurrence (a self-loop),in addition to the outer recurrence of the RNN. Each cell has the same inputs andoutputs as an ordinary recurrent network, but also has more parameters and asystem of gating units that controls the ﬂow of information.
% LSTM networks have been shown to learn long-term dependencies more easilythan the simple recurrent architectures, ﬁrst on artiﬁcial datasets designed fortesting the ability to learn long-term dependencies (Bengio et al., 1994; Hochreiterand Schmidhuber, 1997; Hochreiter et al., 2001), then on challenging sequenceprocessing tasks where state-of-the-art performance was obtained (Graves, 2012;Graves et al., 2013; Sutskever et al., 2014). Variants and alternatives to the LSTMthat have been studied and used are discussed next.

% 10.10.2 Other Gated RNNs
% Which pieces of the LSTM architecture are actually necessary? What othersuccessful architectures could be designed that allow the network to dynamicallycontrol the time scale and forgetting behavior of diﬀerent units?Some answers to these questions are given with the recent work on gated RNNs,whose units are also known as gated recurrent units, or GRUs (Cho et al., 2014b;Chung et al., 2014, 2015a; Jozefowicz et al., 2015; Chrupala et al., 2015). The maindiﬀerence with the LSTM is that a single gating unit simultaneously controls theforgetting factor and the decision to update the state unit.

% ---
% Chapter 11
% Practical Methodology
% https://www.deeplearningbook.org/contents/guidelines.html

% 11.1 Performance Metrics
% [...] As described in section 5.1.2, it iscommon to measure the accuracy, or equivalently, the error rate, of a system.However, many applications require more advanced metrics.

% For example, we might design a medical test for a rare disease. Suppose that only one in every million people has this disease. We can easily achieve 99.9999 percent accuracy on the detection task, by simply hard coding the classiﬁerto always report that the disease is absent. Clearly, accuracy is a poor way tocharacterize the performance of such a system. One way to solve this problem isto instead measure \textbf{precision} and \textbf{recall}. 

% -> Precision is the fraction of detections reported by the model that were correct, while recall is the fraction of true events that were detected. 

% A detector that says no one has the disease would achieve perfect precision, but zero recall. A detector that says everyone has the disease would achieve perfect recall, but precision equal to the percentage of people who have the disease (0.0001 percent in our example of a disease that only one people in a million have). When using precision and recall, it is common to plot a \textbf{PR curve}, with precision on the y-axis and recall on the x-axis. The classiﬁer generates a score that is higher if the event to be detected occurred.

% In many cases, we wish to summarize the performance of the classiﬁer with a single number rather than a curve. To do so, we can convert precision p and recall r into an \textbf{F-score} given by:
%     F = 2pr / p + r.
% Another option is to report the total area lying beneath the PR curve.

% \textbf{Coverage} is the fraction of examples for which the machine learning system is able to produce a response. It is possible to trade coverage for accuracy. One can always obtain 100 percent accuracy by refusing to process any example, but this reduces the coverage to 0 percent.


% 11.2 Default Baseline Models
% A reasonable choice of optimization algorithm is SGD with momentum with a decaying learning rate (popular decay schemes that perform better or worse on diﬀerent problems include decaying linearly until reaching a ﬁxed minimum learning rate, decaying exponentially, or decreasing the learning rate by a factor of 2–10 each time validation error plateaus). Another reasonable alternative is Adam. 
% Batch normalization can have a dramatic eﬀect on optimization performance,especially for convolutional networks and networks with sigmoidal nonlinearities. While it is reasonable to omit batch normalization from the very ﬁrst baseline, itshould be introduced quickly if optimization appears to be problematic.
% Unless your training set contains tens of millions of examples or more, youshould include some mild forms of regularization from the start. 
% Early stopping should be used almost universally. 
% Dropout is an excellent regularizer that is easy to implement and compatible with many models and training algorithms. 
% Batch normalization also sometimes reduces generalization error and allows dropout to be omitted, because of the noise in the estimate of the statistics used to normalize each variable.

% 11.3 Determining Whether to Gather More Data

% 11.4 Selecting Hyperparameters
% There are two basic approaches to choosing these hyperparameters: choosingthem manually and choosing them automatically. Choosing the hyperparameters manually requires understanding what the hyperparameters do and how machinelearning models achieve good generalization. Automatic hyperparameter selectionalgorithms greatly reduce the need to understand these ideas, but they are oftenmuch more computationally costly

% 11.4.1 Manual Hyperparameter Tuning
% To set hyperparameters manually, one must understand the relationship betweenhyperparameters, training error, generalization error and computational resources(memory and runtime). This means establishing a solid foundation on the funda-mental ideas concerning the eﬀective capacity of a learning algorithm, as describedin chapter 5.
% The primary goal of manual hyperparameter search is to adjust the eﬀectivecapacity of the model to match the complexity of the task. Eﬀective capacityis constrained by three factors: the representational capacity of the model, theability of the learning algorithm to successfully minimize the cost function used totrain the model, and the degree to which the cost function and training procedureregularize the model. A model with more layers and more hidden units per layer hashigher representational capacity—it is capable of representing more complicatedfunctions. It cannot necessarily learn all these functions though, if the trainingalgorithm cannot discover that certain functions do a good job of minimizing thetraining cost, or if regularization terms such as weight decay forbid some of thesefunctions.

% The \textbf{learning} rate is perhaps the most important hyperparameter. If youhave time to tune only one hyperparameter, tune the learning rate. It con-trols the eﬀective capacity of the model in a more complicated way than otherhyperparameters—the eﬀective capacity of the model is highest when the learningrate is correct for the optimization problem, not when the learning rate is especiallylarge or especially small. The learning rate has a U-shaped curve for training error,illustrated in ﬁgure 11.1. When the learning rate is too large, gradient descentcan inadvertently increase rather than decrease the training error. In the idealizedquadratic case, this occurs if the learning rate is at least twice as large as itsoptimal value (LeCun et al., 1998a). When the learning rate is too small, trainingis not only slower but may become permanently stuck with a high training error.This eﬀect is poorly understood (it would not happen for a convex loss function).

% > Tuning the parameters other than the learning rate requires monitoring bothtraining and test error to diagnose whether your model is overﬁtting or underﬁtting,then adjusting its capacity appropriately.


% 11.4.2 Automatic Hyperparameter Optimization Algorithms
% 11.4.3 Grid Search
% When there are three or fewer hyperparameters, the common practice is to performgrid search. For each hyperparameter, the user selects a small ﬁnite set ofvalues to explore. The grid search algorithm then trains a model for every jointspeciﬁcation of hyperparameter values in the Cartesian product of the set of valuesfor each individual hyperparameter. The experiment that yields the best validationset error is then chosen as having found the best hyperparameters. See the left ofﬁgure 11.2 for an illustration of a grid of hyperparameter values.

% How should the lists of values to search over be chosen? In the case of numerical(ordered) hyperparameters, the smallest and largest element of each list is chosen conservatively, based on prior experience with similar experiments, to make surethat the optimal value is likely to be in the selected range. Typically, a grid searchinvolves picking values approximately on a logarithmic scale, e.g., a learning ratetaken within the set{0.1,0.01,0.001,0.0001,0.00001}, or a number of hidden unitstaken with the set {50, 100, 200, 500, 1000, 2000}.
% Grid search usually performs best when it is performed repeatedly. For example,suppose that we ran a grid search over a hyperparameterαusing values of{−1,0,1}.If the best value found is 1, then we underestimated the range in which the bestαlies and should shift the grid and run another search withαin, for example,{1,2,3}. If we ﬁnd that the best value ofαis 0, then we may wish to reﬁne our estimate by zooming in and running a grid search over {−0.1, 0, 0.1}.

% 11.4.4 Random Search
% A random search proceeds as follows. First we deﬁne a marginal distributionfor each hyperparameter, for example, a Bernoulli or multinoulli for binary ordiscrete hyperparameters, or a uniform distribution on a log-scale for positivereal-valued hyperparameters. For example,
% log_learning_rate ∼ u(−1, −5), 
% learning_rate = 10log_learning_rate, 

% whereu(a, b) indicates a sample of the uniform distribution in the interval (a, b).Similarly thelog_number_of_hidden_unitsmay be sampled fromu(log(50),log(2000)).
% Unlike in a grid search, we should not discretize or bin the values of the hy-perparameters, so that we can explore a larger set of values and avoid additionalcomputational cost. In fact, as illustrated in ﬁgure 11.2, a random search can beexponentially more eﬃcient than a grid search, when there are several hyperpa-rameters that do not strongly aﬀect the performance measure. This is studied atlength in Bergstra and Bengio (2012), who found that random search reduces thevalidation set error much faster than grid search, in terms of the number of trialsrun by each method.
% As with grid search, we may often want to run repeated versions of randomsearch, to reﬁne the search based on the results of the ﬁrst run.

% 11.4.5 Model-Based Hyperparameter Optimization
% The search for good hyperparameters can be cast as an optimization problem.The decision variables are the hyperparameters. The cost to be optimized is thevalidation set error that results from training using these hyperparameters. 
% [...] we can build a model of the validationset error, then propose new hyperparameter guesses by performing optimizationwithin this model. Most model-based algorithms for hyperparameter search use aBayesian regression model to estimate both the expected value of the validation seterror for each hyperparameter and the uncertainty around this expectation. Opti-mization thus involves a trade-oﬀ between exploration (proposing hyperparametersfor that there is high uncertainty, which may lead to a large improvement but mayalso perform poorly) and exploitation.

