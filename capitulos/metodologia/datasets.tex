\section{Criação do \textit{dataset}}
\label{sec:metodologia-datasets}

O primeiro passo para desenvolver e avaliar a abordagem proposta consiste em estabelecer um \textit{dataset} que viabilize isso. Como a proposta apresentada aqui é nova, não há \textit{dataset}s diretamente compatíveis com ela e, por este motivo, optaremos por derivar um novo a partir de outro já existente -- o \acrfull{asllvd}.

O \acrshort{asllvd}~\cite{athitsos-2008-asllvd,neidle-2012-asllvd} é um \textit{dataset} público\footnote{Disponível em \url{http://www.bu.edu/asllrp/av/dai-asllvd.html}} amplo da \acrshort{asl} que contém aproximadamente 2.745 sinais representados em cerca de 9.763 sequências de vídeo. Esses sinais são articulados por indivíduos Surdos nativos na língua e foram capturados por meio de quatro câmeras distintas sincronizadas: uma visão frontal em alta resolução a meia velocidade, outra visão frontal, uma visão lateral e uma visão da face, conforme ilustrado na \autoref{fig:asllvd-example}.

\begin{figure}[ht!]
    \centering
    \caption{\textmd{Exemplo de três perspectivas capturadas pelo \acrshort{asllvd} para o sinal MERRY-GO-ROUND.}}
    \subcaptionbox{Vista frontal \label{subfig:asllvd-example-front}
    }{
        \includegraphics[width=0.25\textwidth]{capitulos/metodologia/imagens/asllvd_example_front}
    }%
    % \hfill
    \subcaptionbox{Vista lateral \label{subfig:asllvd-example-side}
    }{
        \includegraphics[width=0.25\textwidth]{capitulos/metodologia/imagens/asllvd_example_side}
    }%
    % \hfill
    \subcaptionbox{Vista da face \label{subfig:asllvd-example-close}
    }{
        \includegraphics[width=0.25\textwidth]{capitulos/metodologia/imagens/asllvd_example_close}
    }%
    \nomefonte[p. 2]{athitsos-2008-asllvd}
    \label{fig:asllvd-example}
\end{figure}


Para que fosse possível extrair \textit{features} no formato de parâmetros fonológicos a partir das amostras do \acrshort{asllvd}, que são compostas essencialmente de sequências de vídeos com frames em RGB, tivemos que realizar um processo de duas etapas.

Na primeira, realizamos a estimativa das coordenadas dos esqueletos dos indivíduos para duas das perspectivas fornecidas para as amostras, os quais foram combinados em seguida para compor um esqueleto tridimensional final. A saída dessa etapa deu origem ao \textit{dataset} intermediário denominado ASL-Skeleton3D. 
Na segunda, aplicamos um conjunto de operações algébricas sob esse esqueleto tridimensional para finalmente computar nossos parâmetros fonológicos, processo esse que originou o \textit{dataset} ASL-Phono.

% Para computar parâmetros fonológicos a partir dos frames das amostras do \acrshort{asllvd}, compostos essencialmente de imagens RGB bidimensionais, precisamos realizar um processo de duas etapas: primeiro, estimamos as coordenadas 2D dos esqueletos dos sinalizadores para duas câmeras distintas, frame-a-frame, e as combinamos para projetar um esqueleto no espaço 3D -- isso deu origem ao \textit{dataset} intermediário chamado ASL-Skeleton3D; em seguida, aplicamos um conjunto de operações algébricas sob o esqueleto 3D para calcular os parâmetros fonológicos -- o que gerou assim o nosso \textit{dataset} final chamado ASL-Phono.

Esse processo de extração de \textit{features} envolveu alguns desafios relevantes, dentre os quais podemos enumerar:

\begin{enumerate}
    \item Definição de uma estratégia para representar indivíduos no espaço tridimensional utilizando apenas frames de vídeo bidimensionais simples, bem como para contornar a ausência de perspectivas ou a baixa qualidade de algumas dessas amostras;

    \item Estabelecimento de um subconjunto inicial de atributos fonológicos capaz de capturar e representar variações significativas na articulação dos sinais, e que ao mesmo tempo pudessem ser modelados computacionalmente nessa primeira iteração da abordagem proposta.

    \item Identificação de técnicas matemáticas e medidas antropométricas que pudessem fundamentar a modelagem e o cálculo desses atributos.

    \item Demanda por recursos computacionais significativos para processar duas perspectivas distintas para cada uma das quase 10.000 amostras contidas em cada \textit{dataset}. Em média, isso consumiu cerca de 40 horas contínuas de processamento distribuído com GPUs e gerou mais de 1 TB de dados cada vez que os \textit{dataset}s precisaram ser re-gerados.
\end{enumerate}


% Dataset 3d
\input{capitulos/metodologia/datasets_3d}

% Dataset phono
\input{capitulos/metodologia/datasets_phono}
