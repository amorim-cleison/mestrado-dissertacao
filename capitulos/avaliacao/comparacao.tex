% considerações importantes
% - são pesquisas que adotam abordagens que utilizam dados brutos (RGB, coordenadas)
% - muitas delas adotam recortes das mãos apenas
% - não consideram a fonologia
% - utilizam um número de sinais limitados, com o intuito de reduzir a complexidade -- consequentemente
%     - isso contribui para que eles obtenham acurácias maiores
%     - contudo, limita o problema perante o contexto real -- uma vez que a afasta da realidade da língua

% insights [ASLLVD tem 2.745 sinais]
% - ao comparar nossos experimentos com todos os trabalhos acima, de uma forma generalizada, vemos que o desempenho dos encoder-decoders (por volta dos 46%) foi mediano com relação aos demais (que em vários deles alcançaram marcas de 70%, 85% e até 93,30%)
% - no caso do transformer, seu desempenho posicionou-se superior aos demais trabalhos
% ------------
% - contudo, considerando que maioria deles modela apenas vocabulários pequenos [que correspondem em média a 2% do vocabulário disponível no ASLLVD (que é o que utilizamos aqui) e não ultrapassam 13% dele], nota-se que 
% - no entanto, se considerarmos apenas aqueles trabalhos que consideraram todos os sinais do ASLLVD (que seria um vocabulário de complexidade equivalente à nossa), percebemos que mesmo os encoder-decores apresentaram um salto importante, ficando em torno dos 46% de acurácia (quando até então observava-se por volta de 16%)
%   - se olharmos o transformer, esse salto é ainda mais expressivo




Para comparar os resultados obtidos neste trabalho, selecionamos algumas pesquisas que também adotaram o \acrshort{asllvd} para realizar o reconhecimento dos sinais.
A maioria deles, no entanto, limita-se aos segmentos das mãos para isso e também utiliza como dados de entrada coordenadas 3D ou imagens RGB dos frames das amostras, as quais são processadas por técnicas de visão computacional.
Além disso, eles comumente selecionam subconjuntos menores de sinais ao invés de considerar o \textit{dataset} completo, como fizemos aqui -- isso contribui para acurácias maiores mas limitá-os perante o contexto real de aplicação da língua.


\begin{itemize}    
    % tipo de dado bruto?
    % recorte de mãos x corpo inteiro?

    \item \citeonline{theodorakis-2014-dynamic-static} adotam técnicas não-supervisionadas combinadas com \acrfull{hmm} para gerar subunidades (denominadas 2-S-U) de movimento e pausa da articulação dos sinais a partir dos frames, que são aplicadas a um subconjunto de 97 sinais. 
    % processa frames de vídeo / foco nas mãos

    \item \citeonline{lim-2016-bhof} introduzem o \textit{Block-based Histogram of Optical Flow} (ou Histograma Baseado em Blocos de Fluxo Óptico) (BHOF), que concentra-se nos segmentos das mãos extraídos a partir dos frames das amostras, para reconhecer os sinais. Eles selecionaram um subconjunto de 20 sinais do \acrshort{asllvd} e apresentaram uma comparação dos resultados com outras técnicas como \acrfull{mei}, \acrfull{mhi}, \acrfull{pca} e \acrfull{hof}.
    % processa frames de vídeo / recorta as mãos (remove outras partes)

    \item \citeonline{metaxas-2018-linguistically} agregam \textit{features} geradas por diferentes técnicas de aprendizagem de máquina para parâmetros dos sinais e aplicam-nas como entrada de um modelo baseado em \textit{Hidden Conditional Ordinal Random Fields} (HCORF) para reconhecer um subconjunto de 350 sinais.
    % extrai features: a) handshapes (start and end); b) number of hands; c) 3D upper body locations, movements of the hands and arms, and distance between the hands; d) facial features (include 66 points from 3D estimates for the forehead, ear, eye, nose, and mouth regions, and their velocities across frames) and head movements; e) contact (extracted from our 3D face and upper body movement estimation, and relate to the possibilities of the hand touching specific parts of the head or body).
    % processa frames de video para extrair features de nivel maior / considera mix de features e coordenadas 3D

    \item \citeonline{lim-2019-isolated-slr-cnn-hei} introduzem a representação \textit{Hand Energy Image} (ou Imagem de Energia da Mão) (HEI) que é utilizada como entrada para uma rede \acrshort{cnn} e aplicada aos 20 sinais definidos por \citeonline{lim-2016-bhof}.
    % processa frames de vídeo / recorta as mãos

    \item Por fim, \citeonline{amorim-2019-stgcn-sl} utilizam grafos para modelar as coordenadas 3D do corpo e a dimensão temporal dos movimentos dos indivíduos, os quais foram processados por uma rede \textit{Spatial-Temporal Graph Convolutional Network} (ou Rede Convolucional de Grafo Espaço-Temporal) (ST-GCN) para os 20 sinais definidos por \citeonline{lim-2016-bhof}, mas também para o \textit{dataset} inteiro.
    % coordenadas 2D para grafos / corpo inteiro
\end{itemize}


\input{capitulos/avaliacao/tabelas/comparacao-resultados}


% Algumas comparações (HEI - Hand Energy Image x MEI, MHI) com ASLLVD
% 2020 - Amorim, Macedo, Zanchettin - Spatial-Temporal Graph Convolutional Networks for Sign Language Recognition
%                         accuracy
%     (tudo)              16.48
%     (20 sinais)         61.04

% 2016 - Lim, Tan, Tan - Block-based histogram of optical flow for isolated sign language recognition
%     (20 sinais)
%         MHI             10.00
%         MEI             25.00
%         PCA             45.00
%         HOF             70.00
%         > BHOF          85.00


% 2019 - Lim et al - Isolated sign language recognition using Convolutional Neural Network hand modelling and Hand Energy Image
%     (20 sinais - duas mãos)
%         > Proposed HEI  31.50
%         MEI             25.00
%         MHI             10.00 
%         MEI + MHI       20.00

% 2014 - Dilsizian et al - A New Framework for Sign Language Recognition based on 3D Handshape Identification and Linguistic Modeling
% https://aclanthology.org/L14-1096/
%     >                  81.76

% 2014 - Theodorakis, Pitsikalis, Maragos - Dynamic–static unsupervised sequentiality, statistical subunits and lexicon for sign language recognition
%     (97 signs)
%     > 2-S-U            63.15

% 2018 - Metaxas, Dilsizian, Neidle - Linguistically-driven Framework for Computationally Efficient and Scalable Sign Recognition
% https://par.nsf.gov/servlets/purl/10065369
%     > (350 signs)      93.3

