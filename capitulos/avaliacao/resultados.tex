\section{Análise dos resultados}
\label{sec:avaliacao-resultados}

A \autoref{tab:resultados-modelos} apresenta o desempenho dos modelos utilizados nos experimentos deste trabalho, respectivamente o \textit{Encoder-Decoder} implementado com \acrshort{lstm}, o \textit{Encoder-Decoder} implementado com \acrshort{gru} e o \textit{Transformer}.
Para cada modelo, são listadas as métricas\footnote{
    Uma vez que o reconhecimento de sinais consiste numa classificação multi-classes, os valores das métricas binárias de precisão, \textit{recall} e \textit{F1-score} foram consolidados utilizando-se a média ponderada pelo número de amostras em cada classe.
} de acurácia, precisão, \textit{recall} e \textit{F1-score}, bem como o erro médio calculado.

\input{capitulos/avaliacao/tabelas/resultados-modelos}

Primeiramente, observa-se que os modelos baseados na arquitetura \textit{Encoder-Decoder} apresentaram resultados muito semelhantes entre si de um modo geral.

Dentre eles, percebe-se também que a versão implementada utilizando codificador e decodificador baseados em redes \acrshort{gru} obteve uma pequena vantagem em comparação àquela que utilizou redes \acrshort{lstm} -- ao passo que a primeira alcançou uma acurácia de 88,00\%, a segunda obteve 87,21\%.
O valor das demais métricas e do erro médio computado para ambos os casos replicaram esse mesmo comportamento da acurácia e reforçam tal análise.

Por outro lado, quando analisados os resultados do \textit{Transformer} observa-se um desempenho expressivo com uma acurácia de 100,00\%, a qual excede aquelas apresentadas pelos \textit{Encoder-Decoders} e é reiterada consistentemente pelo valor das demais métricas e também da perda computada.

Isso nos remete à argumentação de \citeonline{wolf-2020-transformers,jurafsky-2022-speech-lang-processing} citada na \autoref{sec:am-ap}, que afirma que essa arquitetura tem se mostrado extremamente bem-sucedida entre tarefas de \acrfull{nlp}, superando arquiteturas como as \acrshortpl{rnn}. Exatamente por isso, elas são atualmente dominantes nessa área.

No contexto deste trabalho, acredita-se que alguns motivos particulares contribuíram para os resultados acima.
Em primeiro lugar, entende-se que a escolha pelo uso da abordagem linguística dos sinais nos permitiu trabalhar num nível semântico muito mais elevado do que seria possível fazer com dados brutos como os pixels de imagens RGB ou coordenadas aleatórias no espaço. Conforme discutido no \autoref{cap:metodos}, essa é uma abordagem comum em tarefas envolvendo linguagem no \acrshort{nlp} e que permite produzir \textit{features} de melhor qualidade para ensinar os modelos acerca da estrutura dessas línguas.

Em segundo lugar, a representação introduzida aqui foi capaz de transformar um conjunto de canais linguísticos complexos dos sinais do \acrshort{asllvd}, em \textit{features} discretas (ou ``palavras'', como aqui as denominamos) mais simples e bem definidas a serem consumidas pelos modelos.
Isso deu origem a um vocabulário que, por sua vez, é muito menos complexo do que aqueles com os quais arquiteturas como o \textit{Transformer} foram originalmente projetadas para lidar. Além disso, nesse processo foi selecionado um número menor de atributos fonológicos da língua de sinais e isso também contribuiu para tornar o vocabulário utilizado aqui ainda mais compacto.

Dessa forma, entende-se que a combinação desse conjunto de \textit{features} semanticamente mais coerentes e simplificadas, com a utilização de modelos robustos no processamento de linguagens como o \textit{Transformer} e os \textit{Encoder-Decoders} foram fatores que conduziram ao desempenho favorável acima.






A \autoref{tab:custo-modelos} apresenta o custo computacional calculado para os modelos adotados neste trabalho ao realizar a inferência de 1.821 amostras durante a etapa de testes. Ela apresenta métricas de tempo e memória para CPU e GPU, bem como o número de \acrfull{flops} para cada um desses modelos.
Ao observar a métrica de tempo de CPU, percebe-se que o \textit{Encoder-Decoder} (\acrshort{gru}) foi aquele que demandou o maior tempo (5,790 segundos), seguido pelo \textit{Transformer} (com 3,378 segundos) e o \textit{Encoder-Decoder} (\acrshort{lstm}) (com 2,912 segundos). 
Com relação à memória de CPU utilizada, os três modelos apresentaram um consumo homogêneo, permanecendo numa faixa entre 335 e 337 MB.

\input{capitulos/avaliacao/tabelas/custo-modelos}

Ao analisar os recursos de GPU, percebe-se que dessa vez o \textit{Encoder-Decoder} (\acrshort{lstm}) demandou o maior tempo (3,101 segundos), seguido pelo \textit{Encoder-Decoder} (\acrshort{gru}) (2,628 segundos) e pelo \textit{Transformer} (com 1,884 segundos). A utilização de memória, por sua vez, manteve-se dentro de uma faixa que oscilou de 24 GB (no caso do \textit{Encoder-Decoder} (\acrshort{gru})) a 30 GB (no caso do \textit{Encoder-Decoder} (\acrshort{lstm})).

Por fim, ao avaliar o número de \acrshort{flops} nota-se que os modelos baseados na arquitetura \textit{Encoder-Decoder} apresentaram o mesmo número de operações. Contudo, no caso do \textit{Transformer} observa-se um número de operações que é aproximadamente cinco vezes maior que os demais modelos.
