\section{Análise dos resultados}
\label{sec:avaliacao-resultados}

A \autoref{tab:resultados-modelos} apresenta o desempenho dos modelos utilizados nos experimentos deste trabalho, respectivamente o \textit{Encoder-Decoder} implementado com \acrshort{lstm}, o \textit{Encoder-Decoder} implementado com \acrshort{gru} e o \textit{Transformer}.
Para cada modelo, são listadas as métricas\footnote{
    Uma vez que o reconhecimento de sinais consiste numa classificação multi-classes, os valores das métricas binárias de precisão, \textit{recall} e \textit{F1-score} foram consolidados utilizando-se a média ponderada pelo número de amostras em cada classe.
} de acurácia, precisão, \textit{recall} e \textit{F1-score}, bem como o erro médio calculado.

\input{capitulos/avaliacao/tabelas/resultados-modelos}

Primeiramente, observa-se que os modelos baseados na arquitetura \textit{Encoder-Decoder} apresentaram resultados muito semelhantes entre si, apesar de ainda não muito expressivos de um modo geral.

Dentre eles, percebe-se também que a versão implementada utilizando codificador e decodificador baseados em redes \acrshort{gru} obteve uma pequena vantagem em comparação àquela que utilizou redes \acrshort{lstm} -- ao passo que a primeira alcançou uma acurácia de 42,78\%, a segunda obteve 42,56\%.
O valor das demais métricas e do erro médio computado para ambos os casos replicaram esse mesmo comportamento e reforçam tal análise.

Por outro lado, quando analisados os resultados do \textit{Transformer} observa-se um desempenho bastante expressivo com uma acurácia de 99,56\%, a qual excede aquelas apresentadas pelos \textit{Encoder-Decoders} e é reiterada consistentemente pelo valor das demais métricas e também do erro médio computado.

Isso nos remete à argumentação de \citeonline{wolf-2020-transformers,jurafsky-2022-speech-lang-processing} citada na \autoref{sec:am-ap}, que afirma que essa arquitetura tem se mostrado extremamente bem-sucedida entre tarefas de \acrfull{nlp}, superando arquiteturas como as \acrshortpl{rnn}. Exatamente por isso, elas são atualmente dominantes nessa área.

No contexto deste trabalho, acredita-se que alguns motivos particulares contribuíram para os resultados acima.
Em primeiro lugar, entende-se que a escolha pelo uso da abordagem linguística dos sinais nos permitiu trabalhar num nível semântico muito mais elevado do que seria possível fazer com dados brutos como os pixels de imagens RGB ou coordenadas aleatórias no espaço. Conforme discutido no \autoref{cap:metodos}, essa é uma abordagem comum em tarefas envolvendo linguagem no \acrshort{nlp} e que permite produzir \textit{features} de melhor qualidade para ensinar os modelos acerca da estrutura dessas línguas.

Em segundo lugar, a representação introduzida aqui foi capaz de transformar um conjunto de canais linguísticos complexos dos sinais do \acrshort{asllvd}, em \textit{features} discretas (ou ``palavras'', como aqui as denominamos) mais simples e bem definidas a serem consumidas pelos modelos.
Isso deu origem a um vocabulário que, por sua vez, é muito menos complexo do que aqueles com os quais arquiteturas como o \textit{Transformer} foram originalmente projetadas para lidar. Além disso, nesse processo foi selecionado um número menor de atributos fonológicos da língua de sinais e isso também contribuiu para tornar o vocabulário utilizado aqui ainda mais compacto.

Dessa forma, entende-se que a combinação desse conjunto de \textit{features} semanticamente mais coerentes e simplificadas, com a utilização de modelos robustos no processamento de linguagens como o \textit{Transformer} foram fatores que conduziram ao desempenho favorável acima.
Contudo, para os \textit{Encoder-Decoders} esse desempenho foi mais modesto.


A \autoref{tab:custo-modelos} apresenta o custo computacional aferido para os modelos durante a etapa de testes que, por sua vez, contempla a inferência dos sinais para as 1.821 amostras que compõem o respectivo subconjunto de testes.
Na tabela, são listadas as métricas de tempo de processamento (em segundos) e memória (em \acrfullpl{mb}) consumidas em ambas CPU e GPU, além do número de \acrfull{flops} (em M\acrshort{flops}, que corresponde a \(1\times 10^6\) \acrshort{flops}).

\input{capitulos/avaliacao/tabelas/custo-modelos}


Ao analisar a métrica de tempo de processamento em CPU, percebe-se que o \textit{Encoder-Decoder} (\acrshort{gru}) foi o modelo com o maior consumo (2,032 segundos), seguido pelo \textit{Transformer} (1,883 segundos) e pelo \textit{Encoder-Decoder} (\acrshort{lstm}) (1,609 segundos).
No entanto, quando considerado o processamento em GPU, o \textit{Transformer} passa a figurar como aquele com o maior tempo (0,687 segundos), seguido pelo \textit{Encoder-Decoder} (\acrshort{gru}) (0,478 segundos) e pelo \textit{Encoder-Decoder} (\acrshort{lstm}) (0,249 segundos).

Com relação ao consumo de memória de CPU, observa-se que os três modelos apresentaram números muito semelhantes entre si (em torno de 50 \acrshort{mb}) -- sendo o consumo do \textit{Transformer} de 50,615 \acrshort{mb}, o do \textit{Encoder-Decoder} (\acrshort{gru}) de 50,327 \acrshort{mb} e o do \textit{Encoder-Decoder} (\acrshort{lstm}) de 50,326 \acrshort{mb}.
Em contrapartida, ao observar a memória de GPU constata-se uma diferença mais significativa, onde o \textit{Transformer} apresenta-se com o consumo mais elevado (de aproximadamente 7,089 \acrshort{gb}) e é seguido pelo \textit{Encoder-Decoder} (\acrshort{gru}) (com aproximadamente 3,605 \acrshort{gb}) e pelo \textit{Encoder-Decoder} (\acrshort{lstm}) (com cerca de 2,164 \acrshort{gb}).

Finalmente, ao analisar o número de operações executadas, observa-se que o \textit{Transformer} realizou uma quantidade significativamente maior que os demais modelos (aproximadamente 656.413 M\acrshort{flops}). 
Na sequência, aparecem o \textit{Encoder-Decoder} (\acrshort{gru}) com 36.791 M\acrshort{flops} e o \textit{Encoder-Decoder} (\acrshort{lstm}) (com 10.758 M\acrshort{flops}).
Essa diferença ilustra de maneira mais concreta a discussão acima que ressalta que a arquitetura \textit{Transformer} apresenta um grau elevado de complexidade e robustez, sobretudo quando comparado às demais arquiteturas \textit{Encoder-Decoders} utilizadas neste trabalho.
