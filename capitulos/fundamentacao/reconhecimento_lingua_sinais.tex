\section{Reconhecimento de língua de sinais}
\label{sec:slr}

% - Introduction (what is it?)
Conforme introduzido anteriormente, o \acrfull{slr} é uma área de pesquisa colaborativa que envolve subáreas da \acrfull{ia} (como o reconhecimento de padrões, visão computacional e processamento de linguagem natural) e linguística. Seu objetivo é construir métodos e algoritmos para identificar sinais produzidos e perceber seu significado~\cite{wadhawan-2019-slr-literature-review}. 

\citeonline{papastratis-2021-ai-technologies-sl,rastgoo-2021-slr-deep-survey,bragg-2019-slr-interdisciplinary} acreditam que tecnologias como esta podem desempenhar um papel importante na quebra das barreiras de comunicação dos Surdos com a comunidade de maioria ouvinte, contribuindo significativamente para sua inclusão em sociedade. Avanços recentes em tecnologias de sensoriamento e algoritmos de \acrshort{ia} abriram caminho para o desenvolvimento de várias aplicações que visam atender às necessidades desses indivíduos. Além disso, o advento das abordagens de aprendizado profundo fez com que os algoritmos de \acrshort{slr} enfrentasse uma melhoria significativa de precisão, nos últimos anos~\cite{papastratis-2021-ai-technologies-sl,rastgoo-2021-slr-deep-survey,bragg-2019-slr-interdisciplinary}.

A comunidade de pesquisa há muito identifica a necessidade de desenvolver tecnologias de linguagem de sinais para facilitar a comunicação e a inclusão social dos Surdos porém, apenas recentemente, a \acrshort{slr} tem recebido uma atenção mais significativa quanto ao número de estudos publicados e de \textit{datasets} disponíveis~\cite{papastratis-2021-ai-technologies-sl,koller-2020-quantitative-survey-slr}.
Essa necessidade evidencia-se pela observação de \citeonline{rastgoo-2021-slr-deep-survey,bragg-2019-slr-interdisciplinary}, que ressalta que a maioria das tecnologias de comunicação são projetadas para suportar a linguagem falada ou escrita, mas excluem a língua de sinais -- e, consequentemente os Surdos. Enquanto ferramentas de comunicação como WhatsApp, Telegram e iMessage tornaram-se parte importante de nossas vidas, os Surdos enfrentam limitações para utilizá-las~\cite{rastgoo-2021-slr-deep-survey,bragg-2019-slr-interdisciplinary}.



% \cite{koller-2020-quantitative-survey-slr}
% Since recently, automatic sign language recognition experiences significantly more attention by the community. The number of published studies, but also the quantity of available data sets is increasing. [...]


% \cite{rastgoo-2021-slr-deep-survey}
% [...] visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years.
% While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved.

% Sign language recognition would help break down the barriers for sign language users in society. Most of the communication technologies have been developed to support spoken or written language (which excludes sign language). While communication technologies and tools such as Imo (Pagebites, 2018) and WhatsApp (Acton & Koum, 2009) have become an important part of our life, deaf people have many problems for using these technologies.
% Daily communication of the deaf community with the hearing majority community can be facilitated using these technologies. As a result, sign language, as a structural form of the hand gestures involving visual motions and signs, is used as a communication system to help the deaf and speech-impaired community for daily interaction.

% [...] With the advent of deep learning approaches, sign language recognition area have faced up a significant accuracy improvement in recent years.


% \cite{bragg-2019-slr-interdisciplinary}
% Sign language recognition, generation, and translation is a research area with high potential impact. (For brevity, we refer to these three related topics as “sign language processing” throughout this paper.) According to the World Federation of the Deaf, there are over 300 sign languages used around the world, and 70 million deaf people using them [89]. Sign languages, like all languages, are naturally evolved, highly structured systems governed by a set of linguistic rules. They are distinct from spoken languages – i.e., American Sign Language (ASL) is not a manual form of English – and do not have standard written forms. However, the vast majority of communications technologies are designed to support spoken or written language (which excludes sign languages), and most hearing people do not know a sign language. As a result, many communication barriers exist for deaf sign language users.
% Sign language processing would help break down these barriers for sign language users. These technologies would make voice-activated services newly accessible to deaf sign language users – for example, enabling the use of personal assistants (e.g., Siri and Alexa) by training them to respond to people signing. They would also enable the use of text-based systems – for example by translating signed content into written queries for a search engine, or automatically replacing displayed text with sign language videos. Other possibilities include automatic transcription of signed content, which would enable indexing and search of sign language videos, real-time interpreting when human interpreters are not available, and many educational tools and applications.
% Current research in sign language processing occurs in disciplinary silos, and as a result does not address the problem comprehensively. For example, there are many computer science publications presenting algorithms for recognizing (and less frequently translating) signed content. The teams creating these algorithms often lack Deaf members with lived experience of the problems the technology could or should solve, and lack knowledge of the linguistic complexities of the language for which their algorithms must account. The algorithms are also often trained on datasets that do not reflect real-world use cases. As a result, such single-disciplinary approaches to sign language processing have limited real-world value [39].
% To overcome these problems, we argue for an interdisciplinary approach to sign language processing. Deaf studies must be included in order to understand the community that the technology is built to serve. Linguistics is essential for identifying the structures of sign languages that algorithms must handle. Natural Language Processing (NLP) and machine translation (MT) provide powerful methods for modeling, analyzing, and translating. Computer vision is required for detecting signed content, and computer graphics are required for generating signed content. Finally, Human-Computer Interaction (HCI) and design are essential for creating end-to-end systems that meet the community’s needs and integrate into people’s lives.


% \cite{wadhawan-2019-slr-literature-review}
% (repeated / introduction) Sign language recognition is a collaborative research area which involves pattern matching, computer vision, natural language processing, and linguistics. Its objective is to build various methods and algorithms in order to identify already produced signs and to perceive their meaning. Sign language recognition systems are Human Computer Interaction (HCI) based systems that are designed to enable effective and engaging interaction. These system follows a multidisciplinary approach of data acquisition, SL technology, SL testing and SL linguistics. Such a system can be deployed in public services like hotels, railways, resorts, banks, offices etc. to enable hearing impaired people learn new concepts and facts and to control emotional behavior [1].


% \cite{papastratis-2021-ai-technologies-sl}
% AI technologies can play an important role in breaking down the communication barriers of deaf or hearing-impaired people with other communities, contributing significantly to their social inclusion. Recent advances in both sensing technologies and AI algorithms have paved the way for the development of various applications aiming at fulfilling the needs of deaf and hearing-impaired communities. 

% The research community has long identified the need for developing sign language technologies to facilitate the communication and social inclusion of hearing-impaired people. Although the development of such technologies can be really challenging due to the existence of numerous sign languages and the lack of large annotated datasets, the recent advances in AI and machine learning have played a significant role towards automating and enhancing such technologies.

% Sign language recognition (SLR) involves the development of powerful machine learning algorithms to robustly classify human articulations to isolated signs or continuous sentences. Current limitations in SLR lie in the lack of large annotated datasets that greatly affect the accuracy and generalization ability of SLR methods, as well as the difficulty in identifying sign boundaries in continuous SLR scenarios.


% \cite{cooper-2011-slr}
% While automatic speech recognition has now advanced to the point of being commercially available, automatic SLR is still in its infancy. Currently all commercial translation services are human based, and therefore expensive, due to the experienced personnel required.
% SLR aims to develop algorithms and methods to correctly identify a sequence of produced signs and to understand their meaning. Many approaches to SLR incorrectly treat the problem as Gesture Recognition (GR). So research has thus far focused on identifying optimal features and classification methods to correctly label a given sign from a set of possible signs. However, sign language is far more than just a collection of well specified gestures.
% Sign languages pose the challenge that they are multi-channel; conveying meaning through many modes at once. While the studies of sign language linguistics are still in their early stages, it is already apparent that this makes many of the techniques used by speech recognition unsuitable for SLR. In addition, publicly available data sets are limited both in quantity and quality, rendering many traditional computer vision learning algorithms inadequate for the task of building classifiers.
% However, even given the lack of translation tools, most public services are not translated into sign. There is no commonly-used, written form of sign language, so all written communication is in the local spoken language.

% SLR has long since advanced beyond classifying isolated signs or alphabet forms  for finger spelling. While the field may continue to draw on the advances in GR the focus has shifted to approach the more linguistic features associated with the challenge. Work has developed on extracting signs from continuous streams and using linguistic grammars to aid recognition. However, there is still much to be learnt from relevant fields such as speech recognition or hand writing recognition.
% In addition, while some have imposed grammatical rules from linguistics, others have looked at data driven approaches, both have their merits since the linguistics of most sign languages are still in their infancy.



% -----------------------------------------------------
% - Panorama (numbers / state of the art)

\subsection{Um breve panorama da área}
\label{sec:slr-breve-panorama}

\citeonline{koller-2020-quantitative-survey-slr} elaborou uma análise quantitativa baseada nos 300 estudos mais relevantes publicados desde 1983, a qual se faz fundamental para compreendermos a evolução da área de \acrshort{slr} bem como o estado da arte atual. Além disso, autores como \citeonline{rastgoo-2021-slr-deep-survey,papastratis-2021-ai-technologies-sl,wadhawan-2019-slr-literature-review} contribuíram nos anos posteriores com pesquisas e revisões literárias que nos permitem esboçar um panorama mais concreto acerca dessa área e aprofundar-nos na breve análise que realizaremos nesta seção.


\subsubsection{Reconhecimento de sinais isolados x contínuos}
\label{sec:slr-sinais-isolados-continuos}

A \autoref{tab:sinais-isolados-continuos} apresenta uma perspectiva detalhada do número de estudos identificados por \citeonline{koller-2020-quantitative-survey-slr} publicados até 2020. Os números estão ordenados na horizontal em intervalos de cinco anos e, na vertical, em grupos que representam o tamanho do vocabulário utilizado na pesquisa. Na parte central, estão os números para os estudos que abordaram o reconhecimento de sinais isolados (onde utiliza-se sinais segmentados); na parte inferior, estão aqueles que abordaram o reconhecimento de sinais contínuos (onde utilizam-se sentenças completas utilizando sinais); na parte superior, estão os números conjuntos para ambos os tipos.

\input{tabelas/fundamentacao/sinais-isolados-continuos}

À primeira vista, percebe-se que o crescimento do número de estudos no decorrer dos anos é mais acelerado para aqueles que utilizam sinais isolados em comparação com aqueles que utilizam sinais contínuos. Isso pode refletir uma maior complexidade de reconhecer sinais contínuos e uma escassez de \textit{datasets} de treinamento disponíveis para isso. 

Além disso, os números revelam que grande maioria dos trabalhos com sinais isolados costumam modelar vocabulários de tamanho limitado (principalmente abaixo de 200 ou 50 sinais), ao passo que trabalhos com sinais contínuos modelam uma diversidade maior de tamanhos de vocabulários e, com o surgimento de novos \textit{datasets} para essa finalidade em 2015, começaram a se concentrar nos vocabulários maiores (acima de 1000 sinais).



\subsubsection{Tipos de dados de entrada}
\label{sec:slr-tipos-dados-entrada}

De acordo com \citeonline{papastratis-2021-ai-technologies-sl}, o tipo de dados utilizados nas pesquisas de reconhecimento de sinais possui uma relação estreita com os tipos de sensores e técnicas que estiveram disponíveis no decorrer dos anos. 

A \autoref{tab:tipos-dados-entrada} mostra o tipo de dados de entrada adotados com relação ao total de estudos, organizados em intervalos de cinco anos. Podemos ver que os dados RGB vem sendo utilizados como alternativa desde as primeiras pesquisas nessa área e tornaram-se populares sobretudo a partir de 2005. O uso de dados de profundidade apenas tornou-se popular com o lançamento do sensor Kinect, em 2010. Luvas eletrônicas também tiveram uma participação expressiva nos anos iniciais e foram gradativamente dando espaço para as luvas coloridas, entre 1995 e 2010, quando iniciou-se uma fase de transição para métodos de processamento baseados em visão. O \textit{mocap}\footnote{
    \textit{Mocap} (\textit{motion capture} ou captura de movimento) consiste na amostragem e captura de movimentos humanos ou de objetos inanimados por meio de equipamentos específicos, que geralmente incluem marcadores ou trajes especiais afixados ao corpo dos indivíduos ou objetos alvos da captura~\cite{kitagawa-2017-mocap}.
}, por sua vez, esteve presente entre 1990 e 2005, mas a dificuldade de acesso a esse equipamento e de aplicação no contexto real da língua de sinais certamente foram barreiras importantes para sua adesão.

\input{tabelas/fundamentacao/dados-entrada-tipos}


Segundo \citeonline{papastratis-2021-ai-technologies-sl}, cada um desses dispositivos apresenta particularidades que os tornam mais ou menos adequadas para diferentes aplicações. Sensores como o Kinect fornecem imagens RGB de alta resolução e dados de profundidade, mas sua precisão é restrita pela distância até o indivíduo rastreado. O uso de múltiplas câmeras RGB pode fornecer resultados altamente precisos, porém demanda uma maior complexidade de processamento e de requisitos computacionais. As luvas eletrônicas, por sua vez, fornecem dados altamente precisos em tempo real, mas a configuração de seus componentes (sensores flexíveis, acelerômetros, giroscópios, etc.) exige um processo de tentativa e erro que é impraticável e demorado. Além disso, os indivíduos tendem a não preferir essas luvas por serem invasivas.



% \cite{papastratis-2021-ai-technologies-sl}

% 3.1. Capturing Sensors
% - sensors
% [...]
% Each of the aforementioned sensor setups for sign language capturing has different  characteristics, which makes it suitable for different applications. Kinect sensors provide high resolution RGB and depth information but their accuracy is restricted by the distance  from the sensors. Leap Motion also requires a small distance between the sensor and the subject, but their low computational requirements enable its usage in real-time applications.
% Multi-camera setups are capable of providing highly accurate results at the expense of increased complexity and computational requirements. A myo armband that can detect EMG and inertial signals is also used in few works but the inertial signals may be distorted by body motions when people are walking. Smartwatches are really popular nowadays and they can also be used for sign language capturing but their output can be quite noisy due to unexpected body movements. Finally, datagloves can provide highly accurate sign language capturing results in real-time. However, the tuning of its components (i.e., flex sensor, accelerometer, gyroscope) may require a trial and error process that is impractical and time-consuming. In addition, signers tend to not prefer datagloves for sign language capturing as they are considered invasive.



Na \autoref{tab:dados-intrusivos-naointrusivos} temos uma comparação dessas técnicas agrupadas em duas categorias: ``não intrusiva'' e ``intrusiva''. Intrusão, nesse contexto, refere-se à necessidade da técnica interferir no indivíduo articulando o sinal para realizar a estimativa de pose corporal ou extração de características para o reconhecimento. Por exemplo, técnicas baseadas em imagens RGB e dados de profundidade são consideradas não intrusivas, enquanto que aquelas utilizando luvas e \textit{mocap}) são consideradas intrusivas. 

Pela tabela, percebe-se claramente uma mudança de paradigma iniciada por volta de 2005 na direção de técnicas de processamento baseadas em visão, que são menos intrusivas. Com isso, os métodos de captura intrusivos anteriormente dominantes foram cada vez menos utilizados e sua prevalência diminuiu de cerca de 70\% para menos de 30\%, e continuou reduzindo ainda mais ao longo do tempo.

\input{tabelas/fundamentacao/dados-entrada-intrusivos-naointrusivos}



\subsubsection{Tipos de parâmetros modelados}
\label{slr-parametros-modelados}

Nesta seção, analisaremos os tipos de parâmetros que foram extraídos a partir dos dados de entrada apresentados anteriormente. Os parâmetros serão classificados entre parâmetros manuais (ou seja, configuração de mão, movimento, locação e orientação) e não-manuais (como cabeça, boca, olhos, piscar de olhos, sobrancelhas e movimento dos olhos). Além disso, também foram rastreados parâmetros comuns que capturam uma visão global dos indivíduos (ou seja, articulações do corpo, imagens RGB, imagens de profundidade e imagens de movimento de fluxo óptico que abrangem o corpo inteiro do indivíduo).

A \autoref{tab:parametros-tipos} mostra a proporção dos tipos de parâmetros utilizados com relação ao total de estudos identificados, agrupados em ordem cronológica de intervalos de cinco anos. 


% É preciso ressaltar que, embora a maioria dos estudos publicados após 2015 utilize um patch de mão recortado como entrada para seus sistemas de reconhecimento, nós o marcamos com o parâmetro de formato de mão. No entanto, usando extratores de recursos baseados em aprendizado profundo, essas entradas de mão podem aprender implicitamente os parâmetros de postura/orientação da mão. Da mesma forma, recursos de entrada global, como entradas de quadro completo, podem ajudar implicitamente a aprender os parâmetros de localização e movimento e, em menor grau, todos os outros parâmetros, bem como a imagem completa, abrange todas as informações disponíveis.


Por meio dela, observa-se uma predominância dos parâmetros manuais 


\input{tabelas/fundamentacao/parametros-tipos}



%A Tabela 6 agrega a localização, movimento, forma e orientação da mão em parâmetros manuais. Cabeça, boca, olhos, piscar de olhos, sobrancelhas e olhares são referidos como parâmetros não manuais. As articulações do corpo, fullframe, profundidade e movimento são todos computados na imagem completa e, portanto, os chamamos de recursos globais. Podemos ver que com vocabulários modelados maiores a tendência vai de recursos manuais para globais (lado esquerdo da Tabela 6), onde estes últimos aumentam de 18% de uso em todos os resultados publicados com vocabulários de até 50 sinais para 62% com vocabulários grandes acima 1000 sinais. O aumento de recursos globais pode ter dois motivos:
% 1. A disponibilidade de articulações do corpo e recursos de imagem de profundidade total com o lançamento do Kinect em 2010.
% 2. A mudança para o aprendizado profundo e a tendência de inserir fullframes em vez da engenharia manual de recursos.

% Ambas as hipóteses podem ser confirmadas olhando para o lado direito da Tabela 6. Lá, vemos que os recursos globais começaram a ganhar força logo após 2010 (lançamento do Kinect) e também coincide com quando o aprendizado profundo para linguagem de sinais decolou em 2015.





% \cite{koller-2020-quantitative-survey-slr}
% - published papers 
%       isolated x continuos signs
%       (static / dynamic signs)
%       single x double handed signs
%       sensors?
%       datasets
% - type of input data
% - modeled parameters
% - accuracy, coverage?


% \cite{rastgoo-2021-slr-deep-survey}
% In this survey, we reviewed the
% proposed models of sign language recognition area using deep learning
% in recent four years based on a proposed taxonomy. Many models have
% been proposed by researchers in recent years. Most of the models have
% used the CNN model for feature extraction from input image due to the
% impressive capabilities of CNN for this goal. In the case of video input,
% RNN, LSTM, and GRU have been used in most of the models to cover
% the sequence information. Also, some models have combined two or
% more approaches in order to boost the recognition accuracy. Moreover,
% different types of input data, such as RGB, depth, thermal, skeleton,
% flow information have been used in the models. Tables 4–9 show the
% proposed models details for sign language recognition in recent four
% years. Furthermore, the pros and cons of these models are presented in
% the Table 10.


% \cite{wadhawan-2019-slr-literature-review}
% 2007 - 2017
% - type of input data
% RQ2: What is the usage of different data acquisition techniques in sign language recognition systems?
% (To identify and analyze various data acquisition devices that are required to capture data for sign language recognition)
% we observed that 55% of the  research work on sign language recognition systems has been done using cameras, followed by 20% using Kinect, 8% using gloves, 7% using arm band, 6% using leap motion and rest 4% using other acquisition devices as shown in Fig. 21a.

% - (static x dynamic signs)
% RQ3: What is the percentage of research carried out on static/dynamic signs in sign language recognition systems?
% (To classify static/dynamic signs based on the research work carried out on SL recognition)
% Fig. 21b depicts that the majority of
% research on sign language recognition systems has been done
% for static signs (45%), followed by dynamic signs (40%) and
% for both static and dynamic signs (15%). 

% - isolated x continuos signs
% RQ4: What is the percentage of research work carried out on the basis of signing mode of SL?
% (To identify different signing modes like isolated and continuous signs)
% it has been observed from Fig. 21c that majority of
% work has been performed on isolated signs (83%), followed
% by continuous signs (12%) and isolated and continuous both
% the signs (5%) sign language recognition systems. 

% - single x double handed signs
% RQ5: What is the percentage of research work carried out on the basis of single and double handed signs?
% (To identify the work done on single and double handed signs)
% we found that 48% of work on sign language
% recognition systems has been performed on single handed
% signs, followed by 20% on double handed signs and 32% on
% both single and double handed signs as shown in Fig. 21d.

% - accuracy, coverage?
% RQ7: What is the accuracy and coverage of existing sign language recognition systems?
% (To identify the recognition rate of existing sign language recognition systems on the trained dataset)
% we observed that for all the sign language systems there are 66%
% of sign language recognition systems who achieved average
% accuracy of greater than 90%, while 23% of the systems have
% accuracy between 80 and 89%. There are only 11% systems
% whose accuracy is less than 80% as shown in Fig. 21f.



% \cite{papastratis-2021-ai-technologies-sl}
% 3. Sign Language Capturing
% Sign language capturing involves the recording of sign gestures using appropriate sensor setups. The purpose is to capture discriminative information from the signs that will allow the study, recognition and 3D representation of signs at later stages. Moreover, sign language capturing enables the construction of large datasets that can be used to accurately train and evaluate machine learning sign language recognition and translation algorithms.

% 3.1. Capturing Sensors
% - sensors
% [...]
% Each of the aforementioned sensor setups for sign language capturing has different  characteristics, which makes it suitable for different applications. Kinect sensors provide high resolution RGB and depth information but their accuracy is restricted by the distance  from the sensors. Leap Motion also requires a small distance between the sensor and the subject, but their low computational requirements enable its usage in real-time applications.
% Multi-camera setups are capable of providing highly accurate results at the expense of increased complexity and computational requirements. A myo armband that can detect EMG and inertial signals is also used in few works but the inertial signals may be distorted by body motions when people are walking. Smartwatches are really popular nowadays and they can also be used for sign language capturing but their output can be quite noisy due to unexpected body movements. Finally, datagloves can provide highly accurate sign language capturing results in real-time. However, the tuning of its components (i.e., flex sensor, accelerometer, gyroscope) may require a trial and error process that is impractical and time-consuming. In addition, signers tend to not prefer datagloves for sign language capturing as they are considered invasive.

% 3.2. Datasets
% - datasets
% Datasets are crucial for the performance of methodologies regarding sign language
% recognition, translation and synthesis and as a result a lot of attention has been drawn
% towards the accurate capturing of signs and their meticulous annotation. The majority
% of the existing publicly available datasets are captured with visual sensors and are
% presented below.
% [...]
% A discussion about the aforementioned datasets can be made at this stage, while a
% detailed overview of the dataset characteristics is provided on Table 1. It can be seen
% that over time datasets become larger in size (i.e., number of samples) with more signers
% involved in them, as well as contain high resolution videos captured under various and
% challenging illumination and background conditions. Moreover, new datasets usually
% include different modalities (i.e., RGB, depth and skeleton). Recording sign language
% videos using many signers is very important, since each person performs signs with
% different speed, body posture and face expression. Moreover, high resolution videos
% capture more clearly small but important details, such as finger movements and face
% expressions, which are crucial cues for sign language understanding. Datasets with videos
% captured under different conditions enable deep networks to extract highly discriminative
% features for sign language classification. As a result, methodologies trained in such datasets
% can obtain greatly enhanced representation and generalization capabilities and achieve high
% recognition performances. Furthermore, although RGB information is the predominant
% modality used for sign language recognition, additional modalities, such as skeleton and
% depth information, can provide complementary information to the RGB modality and
% significantly improve the performance of SLR methods.


% 7. Conclusions and Future Directions
% The aim of this review is to familiarize researchers with sign language technologies and assist them
% towards developing better approaches.
% - sensors
% In the field of sign language capturing, it is essential to select an optimal sensor for
% capturing signs for a task that highly depends on various constraints (e.g., cost, speed,
% accuracy, etc.). For instance, wearable sensors (i.e., gloves) are expensive and capture only
% hand joints and arm movements, while in recognition applications, the user is required
% to use gloves. On the other hand, camera sensors, such as web or smartphone cameras,
% are inexpensive and capture the most substantial information, like the face and the body
% posture, which are crucial for sign language.
% - techniques
% Concerning CSLR approaches, most of the existing works adopt 2D CNNs with
% temporal convolutional networks or recurrent neural networks that use video as input.
% In general, 2D methods have lower training complexity compared to 3D architectures and
% produce better CSLR performance. Moreover, it is experimentally shown that multi-modal
% architectures that utilize optical flow or human pose information, achieve slightly higher
% recognition rates than unimodal methods. In addition, CSLR performance on datasets with
% large vocabularies of more than 1000 words, such as Phoenix-2014, or datasets with unseen
% words on the test sets, such as CSL Split 2 and GSL SD, is far from perfect. Furthermore,
% ISLR methods have been extensively explored and have achieved high recognition rates on
% large-scale datasets. However, they are not suitable for real-life applications since they are
% trained to detect and classify isolated signs on pre-segmented videos.
% Sign language translation methods have shown promising results although they are
% not exhaustively explored. The majority of the SLT methods adopt architectures from the
% field of neural machine translation and video captioning. These approaches are of great
% importance, since they translate sign language into spoken counterparts and can be used
% to facilitate the communication between the Deaf community and other groups. To this
% end, this research field requires additional attention from the research community.
% [...]
% From the chart in Figure 3a, it can be seen that most existing works deal with sign
% language recognition, while sign language capturing and translation methods are still not
% thoroughly explored. It is strongly believed that these research areas should be explored
% more in future works.




% \cite{cooper-2011-slr}
% ---
% 3 Data Acquisition and Feature Extraction
% Acquiring data is the first step in a SLR system. Given that much of the meaning
% in sign language is conveyed through manual features, this has been the area of
% focus of the research up to the present as noted by Ong and Ranganath in their 2005
% survey [82].
% - sensors
% Many early SLR systems used data gloves and accelerometers to acquire specifics
% of the hands. The measurements (x,y,z, orientation, velocity etc) were measured directly
% using a sensor such as the Polhemus tracker [103] or DataGlove [54, 99].
% More often than not, the sensor input was of sufficient discriminatory power that feature
% extraction was bypassed and the measurements used directly as features [34].
% While these techniques gave the advantage of accurate positions, they did not allow
% full natural movement and constricted the mobility of the signer, altering the
% signs performed. Trials with a modified glove-like device, which was less constricting
% [43], attempted to address this problem. However, due to the the prohibitive
% costs of such approaches, the use of vision has become more popular. In the case of
% vision input, a sequence of images are captured from a combination of cameras (e.g.
% monocular [115], stereo [47], orthogonal [90]) or other non-invasive sensors. Segen
% and Kumar [87] used a camera and calibrated light source to compute depth, and
% Feris et al. [30] used a number of external light sources to illuminate a scene and
% then used multi-view geometry to construct a depth image. Starner et al. [91] used
% a front view camera in conjunction with a head mounted camera facing down on
% the subject’s hands to aid recognition. Depth can also be inferred using stereo cameras
% as was done by Munoz-Salinas et al. [72] or by using side/vertical mounted
% cameras as with Vogler and Metaxas [100] or the Boston ASL data set [75]. There
% are several projects which are creating sign language data sets; in Germany there is
% the DGS-Korpus dictionary project collecting data across the country over a 15yr
% period [22] or the similar project on a smaller scale in the UK by the BSL Corpus
% Project [14]. However, these data sets are directed at linguistic research, whereas
% the cross domain European project DictaSign [23] aims to produce a multi-lingual
% data set suitable for both linguists and computer vision scientists.

% 6 Conclusions
% SLR has long since advanced beyond classifying isolated signs or alphabet forms for finger spelling. While the field may continue to draw on the advances in GR the focus has shifted to approach the more linguistic features associated with the challenge. Work has developed on extracting signs from continuous streams and using linguistic grammars to aid recognition. However, there is still much to be learnt from relevant fields such as speech recognition or hand writing recognition.
% In addition, while some have imposed grammatical rules from linguistics, others have looked at data driven approaches, both have their merits since the linguistics of most sign languages are still in their infancy.
% While the community continues to discuss the need for including non-manual features, few have actually done so. Those which have [2, 5], concentrate solely on the facial expressions of sign. There is still much to be explored in the veins of body posture or placement and classifier (hand shape) combinations.
% Finally, to compound all these challenges, there is the issue of signer independence.
% While larger data sets are starting to appear, few allow true tests of signer independence over long continuous sequences. Maybe this is one of the most urgent problems in SLR that of creating data sets which are not only realistic, but also well annotated to facilitate machine learning.
% Despite these problems recent uses of SLR include translation to spoken language, or to another sign language when combined with avatar technology [3, 25].







% -----------------------------------------------------
% - Challenges
%Embora a tendência geral dos modelos propostos indique uma melhora significativa na precisão do reconhecimento de linguagem de sinais, ainda existem alguns desafios que precisam ser resolvidos.
%\cite{rastgoo-2021-slr-deep-survey}


% \cite{cooper-2011-slr}
% [...]
% Although SLR and speech recognition are drastically different in many respects,
% they both suffer from similar issues; co-articulation between signs means that a sign
% will be modified by those either side of it. Inter-signer differences are large; every
% signer has their own style, in the same way that everyone has their own accent or
% handwriting. Also similar to handwriting, signers can be either left hand or right
% hand dominant. For a left handed signer, most signs will be mirrored, but time line
% specific ones will be kept consistent with the cultural ‘left to right’ axis. While it
% is not obvious how best to include these higher level linguistic constructs of the
% language, it is obviously essential if true, continuous SLR is to become reality.
% ---
% 5 Research Frontiers
% There are many facets of SLR which have attracted attention in the computer vision
% community. This section serves to outline the areas which are currently generating
% the most interest due to the challenges they propose. While some of these are recent
% topics, others have been challenging computer vision experts for many years. Offered
% here is a brief overview of the seminal work and the current state of the art in
% each area.

% 5.1 Continuous Sign Recognition
% The majority of work on SLR has been focused on recognising isolated instances of
% signs, this is not applicable to a real world sign language recognition system. The
% task of recognising continuous sign language is complicated primarily by the problem
% that in natural sign language, the transition between signs is not clearly marked
% because the hands will be moving to the starting position of the next sign. This
% is referred to as the movement epenthesis or co-articulation (which borrows from
% speech terminology).
% [...]

% 5.2 Signer Independence
% A major problem relating to recognition is that of applying the system to a signer
% on whom the system has not been trained.
% [...]

% 5.3 Fusing Multi-Modal Sign Data
% From the review of SLR by Ong and Ranganath [82], one of their main observations
% is the lack of attention that non-manual features has received in the literature.
% This is still the case several years on. Much of the information in a sign is conveyed
% through this channel, and particularly there are signs that are identical in respect of
% the manual features and only distinguishable by the non-manual features accompanying
% the sign. The difficulty is identifying exactly which elements are important to
% the sign, and which elements are coincidental. For example, does the blink of the
% signer convey information valuable to the sign, or was the signer simply blinking?
% This problem of identifying the parts of the sign that contains information relevant to
% the understanding of the sign makes SLR a complex problem to solve.
% [...]

% 5.4 Using Linguistics
% The task of recognition is often simplified by forcing the possible word sequence
% to conform to a grammar which limits the potential choices and thereby improves
% recognition rates [91, 104, 12, 45].
% [...]

% 5.5 Generalising to More Complex Corpora
% Due to the lack of adequately labelled data sets, research has turned to weakly supervised
% approaches. Several groups have presented work aligning subtitles with
% signed TV broadcasts.
% [...]






















% ===================================================
% \cite{koller-2020-quantitative-survey-slr}

% 1 Introduction
% Since recently, automatic sign language recognition experiences significantly more attention by the community. The number of published studies, but also the quantity of available data sets is increasing. [...]

% 2 Analysis of the State of the Art
% Figure 1 shows the number of published isolated and continuous recognition results in blocks of five years up until 2020. We see that growth looks exponential for isolated studies, while being close to linear for continuous studies. This may reflect the difficulty of the continuous recognition scenario and also the scarcity of available training corpora. On average it seems that there are at least twice as many studies published using isolated sign language data.
% However, Figure 2, which shows the number of isolated and continuous recognition results aggregated by vocabulary size, reveals that the vast majority of the isolated sign language recognition works model a very limited amount of signs only (i.e. below 50 signs). This is not the case when comparing continuous sign language recognition, where the overall studies more or less evenly spread across all sign vocabularies (with exception of 500-1000 signs due to lack of available corpora)
% > [chart 1]
% > [chart 2]

% 2.1 Type of Employed Input Data
% Table 2 shows in the top part of the type of employed input data across different sizes of modeled vocabulary. The input
% data refers to the data that is consumed by the recognition algorithms to extract features from and perform computation.
% We can observe that RGB is the most popular type of input data both for small and larger scale vocabulary ranges.
% Colored gloves have only ever been applied to small and medium vocabulary tasks and did never get significant attention.
% The lower part of Table 2 shows the type of employed input data relative to all results published in the same range of
% years. We can see that RGB data attracts most attention since 2005. Depth as input modality became only popular
% after the release of the Kinect sensor in 2010. There was one work that employed depth data before [Fujimura and Xia
% Liu, 2006] which had access to early time-of-flight sensors. Colored gloves got some traction between 1995 and 2010,
% which looks like a transition phase from electronic measuring devices to pure vision based processing.
% > [table 2]

% Table 3 displays the input data aggregated into the categories ‘non-intrusive’ and ‘intrusive’. Intrusiveness refers
% to the need to interfere with the recognition subject in order to perform body pose estimation and general feature
% extraction. As such, ‘RGB’ and ‘Depth’ are non-intrusive capturing methods, while ‘Color Glove’, ‘Electronic Glove’
% and ‘Motion Capturing’ are intrusive techniques. As can be seen in Table 3 on the left, intrusives capturing methods can
% be encountered in about one quarter of all experiments with a vocabulary of up to 500 signs. They are more rare in
% larger vocabulary sizes, possibly due to the fact that those have mainly been researched after 2010 (compare Table 1).
% We clearly see a paradigm shift after 2005, when the formerly dominating intrusives capturing methods were less and
% less used and their prevalence decreased from around 70% to less than 30% with a tendency to further reduce over time.
% > [table 2]


% 2.2 Modeled Sign Language Parameters
% We note that hand shape is the most covered parameter, while location and movement are the next popular parameters
% across all vocabulary sizes below 1000 signs. Fullframe features followed by hand shapes are most frequently
% encountered in large vocabulary tasks beyond 1000 signs. The lower part of Table 5 confirms that since 2015
% fullframe features have become the most frequently encountered feature (while being very close to hand shape features).
% Furthermore, it can be noticed that since 2015 hand shape are tackled by a much larger fraction of published results. It
% needs to be pointed out that while most studies that have been published after 2015 employ a cropped hand patch as
% input to their recognition systems, we tagged that with the hand shape parameter. However, using deep learning based
% feature extractors, such hand inputs may implicitly learn hand posture / orientation parameters. Similarly, global input
% features such as fullframe inputs may implicitly help to learn location and movement parameters and, to a lesser degree,
% all other parameters as well as the full image comprises all available information.

% Table 6 aggregates hand location, movement, shape and orientation into manual parameters. Head, mouth, eyes, eye
% blink, eyebrows and eye gaze are referred to as non-manual parameters. Body joints, fullframe, depth and motion are all
% computed on the full image and hence we call them global features. We can see that with larger modeled vocabularies
% the trend goes from manual to global features (left side of Table 6), where the latter increase from 18\% usage across all
% published results with vocabularies of up to 50 signs to 62\% with large vocabularies above 1000 signs. The increase of
% global features may have two reasons:
% 1. The availability of body joints and full depth image features with the release of the Kinect in 2010.
% 2. The shift towards deep learning and trend to input fullframes instead of manual feature engineering.
% Both hypotheses can be confirmed by looking at the right side of Table 6. There, we see that global features started
% gaining traction just after 2010 (release of the Kinect) and also coincides with when deep learning for sign language
% took off in 2015.

% Conclusion and outlook
% Among others, we present following findings in this meta study:
% - While many more studies are published on isolated than on continuous sign language recognition, the majority
% only covers small vocabulary tasks.
% - After 2005 there was a paradigm shift in the community abandoning intrusive capturing methods and embracing non-intrusive methods.
% - Deep learning led the community towards the predominant use of global feature representations that are based on fullframe inputs. Those are particularly more common for larger vocabulary tasks.
% - Non-manual parameters are still very rare in sign language recognition systems, despite their known importance for sign languages [Pfau and Quer, 2010]. No sign recognition work has included eye gaze or blinks yet. Despite being the second most frequently researched sign language, research studies for CSL have hardly incorporated non-manual parameters. DGS is currently the only sign language where non-manuals have been successfully incorporated considering tasks with a vocabulary of at least 200 signs.
% - RWTH-PHOENIX-Weather with a vocabulary of 1080 signs represents the only resource for large vocabulary
% continuous sign language world wide.



% ===================================================
% \cite{bragg-2019-slr-interdisciplinary}

% Sign language recognition, generation, and translation is a
% research area with high potential impact. (For brevity, we
% refer to these three related topics as “sign language processing”
% throughout this paper.) According to the World Federation
% of the Deaf, there are over 300 sign languages used around
% the world, and 70 million deaf people using them [89]. Sign
% languages, like all languages, are naturally evolved, highly
% structured systems governed by a set of linguistic rules. They
% are distinct from spoken languages – i.e., American Sign Language
% (ASL) is not a manual form of English – and do not
% have standard written forms. However, the vast majority of
% communications technologies are designed to support spoken
% or written language (which excludes sign languages), and most
% hearing people do not know a sign language. As a result, many
% communication barriers exist for deaf sign language users.

% Sign language processing would help break down these barriers  for sign language users. These technologies would make voice-activated services newly accessible to deaf sign language users – for example, enabling the use of personal assistants (e.g., Siri and Alexa) by training them to respond to people signing. They would also enable the use of text-based systems – for example by translating signed content into written queries for a search engine, or automatically replacing displayed text with sign language videos. Other possibilities include automatic transcription of signed content, which would enable indexing and search of sign language videos, real-time interpreting when human interpreters are not available, and many educational tools and applications.

% Current research in sign language processing occurs in disciplinary silos, and as a result does not address the problem comprehensively. For example, there are many computer science publications presenting algorithms for recognizing (and less frequently translating) signed content. The teams creating these algorithms often lack Deaf members with lived experience of the problems the technology could or should solve, and lack knowledge of the linguistic complexities of the language for which their algorithms must account. The algorithms are also often trained on datasets that do not reflect real-world use cases. As a result, such single-disciplinary approaches to sign language processing have limited real-world value [39].

% To overcome these problems, we argue for an interdisciplinary approach to sign language processing. Deaf studies must be included in order to understand the community that the technology is built to serve. Linguistics is essential for identifying the structures of sign languages that algorithms must handle. Natural Language Processing (NLP) and machine translation (MT) provide powerful methods for modeling, analyzing, and translating. Computer vision is required for detecting signed content, and computer graphics are required for generating signed content. Finally, Human-Computer Interaction (HCI) and design are essential for creating end-to-end systems that meet the community’s needs and integrate into people’s lives.

% Q1: WHAT IS THE CURRENT LANDSCAPE?
% [...]

% Q2: WHAT ARE THE FIELD’S BIGGEST CHALLENGES?

% Datasets
% Public sign language datasets have shortcomings that limit the power and generalizability of systems trained on them.
% - Size: Modern, data-driven machine learning techniques work best in data-rich scenarios. Success in speech recognition, which in many ways is analogous to sign recognition, has been made possible by training on corpora containing millions of words. In contrast, sign language corpora, which are needed to fuel the development of sign language recognition, are several orders of magnitude smaller, typically containing fewer than 100,000 articulated signs. (See Table 2 for a comparison between speech and sign language datasets.)
% - Continuous Signing: Many existing sign language datasets contain individual signs. Isolated sign training data may be important for certain scenarios (i.e., creating a sign language dictionary), but most real-world use cases of sign language processing involve natural conversational with complete sentences and longer utterances.
% - Native Signers: Many datasets allow novices (i.e., students) to contribute, or contain data scraped from online sources (e.g., YouTube [62]) where signer provenance and skill is unknown. Professional interpreters, who are highly skilled but are often not native signers, are also used in many datasets (e.g., [42]). The act of interpreting also changes the execution (e.g., by simplifying the style and vocabulary, or signing slower for understandability). Datasets of native signers are needed to build models that refect this core user group.
% - Signer Variety: The small size of current signing datasets and over-reliance on content from interpreters mean that current datasets typically lack signer variety. To accurately refect the signing population and realistic recognition scenarios, datasets should include signers that vary by: gender, age, clothing, geography, culture, skin tone, body proportions, disability, fuency, background scenery, lighting conditions, camera quality, and camera angles. It is also crucial to have signer-independent datasets, which allow people to assess generalizability by training and testing on different signers. Datasets must also be generated for different sign languages (i.e., in addition to ASL).

% Recognition & Computer Vision
% Despite the large improvements in recent years, there are still mnay important and unsolved recognition problems, which hinder real-world applicability.
% - Depiction: Depiction refers to visually representing or enacting content in sign languages (see Background & Related Work), and poses unique challenges for recognition and translation. Understanding depiction requires exposure to Deaf culture and linguistics, which the communities driving progress in computer vision generally lack. Sign recognition algorithms are often based on speech recognition, which does not handle depictions (which are uncommon and unimportant in speech). As a result, current techniques cannot handle depictions. It is also diffcult to create depiction annotations. Countless depictions can express the same concept, and annotation systems do not have a standard way to encode this richness.
% - Annotations: Producing sign language annotations, the machine-readable inputs needed for supervised training of AI models, is time consuming and error prone. There is no standardized annotation system or level of annotation granularity. As a result, researchers are prevented from combining annotated datasets to increase power, and must handle low inter-annotator agreement. Annotators must also be trained extensively to reach suffcient profciency in the desired annotation system. Training is expensive, and constrains the set of people who can provide annotations beyond the already restricted set of fuent signers. The lack of a standard written form also prevents learning from naturally generated text – e.g., NLP methods that expect text input, using parallel text corpora to learn corresponding grammar and vocabulary, and more generally leveraging ubiquitous text resources.
% - Generalization: Generalization to unseen situations and individuals is a major diffculty of machine learning, and sign language recognition is no exception. Larger, more diverse datasets are essential for training generalizable models. We outlined key characteristics of such datasets in the prior section on dataset challenges. However, generating such datasets can be extremely time-consuming and expensive.

% Modeling & Natural Language Processing
% The main challenge facing modeling and NLP is the inability to apply powerful methods used for spoken/written languages due to language structure differences and lack of annotations.
% - Structural Complexity: Many MT (machine translation) and NLP methods were developed for spoken/written languages. However, sign languages have a number of structural differences from these languages. These differences mean that straightforward application of MT and NLP methods will fail to capture some aspects of sign languages or simply not work. In particular, many methods assume that one word or concept is executed at a time. However, many sign languages are multi-channel, for instance conveying an object and its description simultaneously. Many methods also assume that context does not change the word being uttered; however, in sign languages, content can be spatially organized and interpretation directly dependent on that spatial context.
% - Annotations: Lack of reliable, large-scale annotations are a barrier to applying powerful MT and NLP methods to sign languages. These methods typically take annotations as input, commonly text. Because sign languages do not have a standard written form or a standard annotation form, we do not have large-scale annotations to feed these methods. Lack of large-scale annotated data is similarly a problem for training recognition systems, as described in the previous section.

% [...]
% Avatars & Computer Graphics
% UI/UX Design


% Q3: WHAT ARE THE CALLS TO ACTION?

% Deaf Involvement
% ---
% Call 1: Involve Deaf team members throughout.
% Deaf involvement and leadership are crucial for designing systems that are useful to users, respecting Deaf ownership of sign languages, and securing adoption.
% ---
% In developing sign language processing, Deaf community involvement is essential at all levels, in order to design systems that actually match user needs, are usable, and to facilitate adoption of the technology. An all-hearing team lacks the lived experience of Deafness, and is removed from the use cases and contexts within which sign language software must function. Even hearing people with strong ties to the Deaf community are not in a position to speak for Deaf needs. Additionally, because of their perceived expertise in Deaf matters, they are especially susceptible to being involved in Deaf-hearing power imbalances. People who do not know a sign language also typically make incorrect assumptions about sign languages – e.g., assuming that a particular gesture always translates to a particular spoken/written word. As a result, all-hearing teams are ill-equipped to design software that will be truly useful.
% It is also important to recognize individual and community freedoms in adopting technology. Pushing a technology can lead to community resentment, as in the case of cochlear implants for many members of sign language communities [104]. Disrespecting the Deaf community’s ownership over sign languages also furthers a history of audism and exclusion, which can result in the Deaf community rejecting the technology. For these reasons, a number of systems built by hearing teams to serve the Deaf community have failed or receive mixed reception (e.g., sign language gloves [39]).
% Deaf contributors are essential at every step of research and development. For example, involvement in the creation, evaluation, and ownership of sign language datasets is paramount to creating high-quality data that accurately represents the community, can address meaningful problems, and avoids cultural appropriation.


% Application Domain
% ---
% Call 2: Focus on real-world applications.
% Sign language processing is appropriate for specifc domains, and the technology has limitations. Datasets, algorithms, interfaces, and overall systems should be built to serve real-world use cases, and account for real-world constraints.
% ---
% There are many different application domains for sign language processing. Situations where an interpreter would be benefcial but is not available are one class of applications. This includes any point of sale, restaurant service, and daily spontaneous interactions (for instance with a landlord, colleagues, or strangers). Developing personal assistant technologies that can respond to sign language is another compelling application area. Each of these scenarios requires different solutions. 
% Furthermore, these different use cases impose unique constraints on every part of the pipeline, including the content, format, and size of training data, the properties of algorithms, as well as the interface design. Successful systems require buy-in from the Deaf community, so ensuring that solutions handle application domains appropriately is essential.
% Technical limitations impact which domains are appropriate to tackle in the near-term, and inform intermediary goals that which will ultimately inform end-to-end systems. Many of these intermediary goals are worth pursuing in and of themselves, and offer bootstrapping benefts toward longer-term goals.


% Interface Design
% [...]


% Datasets
% ---
% Call 4: Create larger, more representative, public video datasets.
% Large datasets with diverse signers are essential for training software to perform well for diverse users. Public availability is important for spurring developments, and for ensuring that the Deaf community has equal ownership.
% ---
% As highlighted throughout this work, few large-scale, publicly available sign language corpora exist. Moreover, the largest public datasets are orders of magnitude smaller than those of comparable felds like speech recognition. The lack of large-scale public datasets shifts the focus from algorithmic and system development to data curation. Establishing large, appropriate corpora would expedite technical innovation.
% In particular, the field would benefit from a larger body of research involving reproducible tasks. Publicly available data and competitive evaluations are needed to create interest, direct research towards the challenges that matter (tackling depiction, generalizing to unseen signers, real-life data), and increase momentum. Furthermore, having open-source implementations of full pipelines would also foster faster adoption.

% There are four main approaches for collecting signing data, each of which has strengths and weaknesses. Developing multiple public data resources that span these four approaches may be necessary in order to balance these tradeoffs.
% 1. Scraping video sites (e.g., YouTube) [...]
% 2. Crowdsourcing data through existing platforms (e.g., Amazon Mechanical Turk) or customized sites (e.g., [14]) [...]
% 3. Bootstrapping, where products are released with limitations and gather data during use, is common to other AI domains (e.g., voice recognition [97]). [...]
% 4. In-lab collection allows for customized, high-end equipment such as high-resolution, high-frame-rate cameras, multiple cameras, depth-cameras, and motion-capture suits. [...]

% Some metadata impacting data utility can only be gathered at the time of capture. In particular, demographics may be important for understanding biases and generalizability of systems trained on the data [44]. Key demographics include signing fuency, language acquisition age, education (level, Deaf vs. mainstream), audiological status, socioeconomic status, gender, race/ethnicity, and geography. Such metadata can also beneft linguistics, Deaf studies, and other disciplines.
% Metadata regarding the data collection process itself (i.e., details enabling replication) are also vital to include so that others can add to the dataset. For example, if a dataset is gathered in the U.S., researchers in other countries could replicate the collection method to increase geographic diversity.


% Annotations
% ---
% Call 5: Standardize the annotation system and develop software for annotation support.
% Annotations are essential to training recognition systems, providing inputs to NLP and MT software, and generating signing avatars. Standardization would support data sharing, expand software compatibility, and help control quality. Annotation support would help improve accuracy, reliability, and cost.
% ---
% A standard annotation system would expedite development of sign language processing. Datasets annotated with the standard system could easily be combined and shared. Software systems built to be compatible with that annotation system would then have much more training data at their disposal. A standard system would also reduce annotation cost and errors. As described earlier, the lack of standardization results in expensive training (and re-training) of annotators, and ambiguous, error-prone annotations.
% Designing the annotation system to be appropriate for everyday reading and writing, or developing a separate standard writing system, would provide addition benefts. With such a system, email clients, text editors, and search engines would become newly usable in sign languages without translating into a spoken/written language. As they write, users would also produce a large annotated sign language corpus of naturally generated content, which could be used to better train models. However, establishing a standard writing system requires the Deaf community to reach consensus on how much of the live language may be abstracted away. Any writing system loses some of the live language (i.e., a transcript of a live speech in English loses pitch, speed, intonation, and emotional expression). Sign languages will be no different.
% Computer-aided annotation software has been proposed (e.g., [29, 33, 24]), but could provide increased support due to recent advances in deep learning applied to sign language recognition. Current sign language modeling techniques could be used to aid the annotation process in terms of both segmenting and transcribing the input video. Aided annotation should leverage advances in modeling whole signs and also sign subunits [73, 72]. Annotation support tools could also alleviate problems with annotating depictions, as they could propose annotations conditioned on the translation and hence circumvent the problem of detailing the iconic nature of these concepts.


% ===================================================
% \cite{rastgoo-2021-slr-deep-survey}

% Abstract
% [...] visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved.

% Introduction
% Sign language recognition would help
% break down the barriers for sign language users in society. Most of the
% communication technologies have been developed to support spoken
% or written language (which excludes sign language). While communication
% technologies and tools such as Imo (Pagebites, 2018) and
% WhatsApp (Acton & Koum, 2009) have become an important part of
% our life, deaf people have many problems for using these technologies.
% Daily communication of the deaf community with the hearing majority
% community can be facilitated using these technologies. As a result, sign
% language, as a structural form of the hand gestures involving visual
% motions and signs, is used as a communication system to help the deaf
% and speech-impaired community for daily interaction.

% Discussion and conclusion
% Sign language and different forms of sign-based communication are
% prominent to large groups in society. With the advent of deep learning
% approaches, sign language recognition area have faced up a significant
% accuracy improvement in recent years. In this survey, we reviewed the
% proposed models of sign language recognition area using deep learning
% in recent four years based on a proposed taxonomy. Many models have
% been proposed by researchers in recent years. Most of the models have
% used the CNN model for feature extraction from input image due to the
% impressive capabilities of CNN for this goal. In the case of video input,
% RNN, LSTM, and GRU have been used in most of the models to cover
% the sequence information. Also, some models have combined two or
% more approaches in order to boost the recognition accuracy. Moreover,
% different types of input data, such as RGB, depth, thermal, skeleton,
% flow information have been used in the models. Tables 4–9 show the
% proposed models details for sign language recognition in recent four
% years. Furthermore, the pros and cons of these models are presented in
% the Table 10.
% [...]

% Table 11
% State-of-the-art models on the datasets corresponding to the sign language and related areas.
% ---
% Dataset   Year    Ref.                Goal    Model   Modality        Results
% ---
% ASLLVD    2019    (Lim et al., 2019)  HS      CNN     dynamic, RGB    31.50

% HP: Hand Pose, HT: Hand Tracking, HD: Hand Detection,
% HSR: Hand Sign Recognition, HG: Hand Gesture, RHR: Real-time Hand
% Recognition.





% ===================================================
% \cite{wadhawan-2019-slr-literature-review}
% Decade -> 2007 - 2017

% Sign language recognition is a collaborative research area
% which involves pattern matching, computer vision, natural
% language processing, and linguistics. Its objective is to build
% various methods and algorithms in order to identify already
% produced signs and to perceive their meaning. Sign language
% recognition systems are Human Computer Interaction (HCI)
% based systems that are designed to enable effective and
% engaging interaction. These system follows a multidisciplinary
% approach of data acquisition, SL technology, SL testing
% and SL linguistics. Such a system can be deployed in public
% services like hotels, railways, resorts, banks, offices etc. to
% enable hearing impaired people learn new concepts and facts
% and to control emotional behavior [1].


% 4 Overall Observation by Considering the Research Work on 
% All Sign Language Recognition Systems

% RQ2: What is the usage of different data acquisition techniques in sign language recognition systems?
% (To identify and analyze various data acquisition devices that are required to capture data for sign language recognition)
% we observed that 55% of the
% research work on sign language recognition systems has
% been done using cameras, followed by 20% using Kinect, 8%
% using gloves, 7% using arm band, 6% using leap motion and
% rest 4% using other acquisition devices as shown in Fig. 21a.

% RQ3: What is the percentage of research carried out on static/dynamic signs in sign language recognition systems?
% (To classify static/dynamic signs based on the research work carried out on SL recognition)
% Fig. 21b depicts that the majority of
% research on sign language recognition systems has been done
% for static signs (45%), followed by dynamic signs (40%) and
% for both static and dynamic signs (15%). 

% RQ4: What is the percentage of research work carried out on the basis of signing mode of SL?
% (To identify different signing modes like isolated and continuous signs)
% it has been observed from Fig. 21c that majority of
% work has been performed on isolated signs (83%), followed
% by continuous signs (12%) and isolated and continuous both
% the signs (5%) sign language recognition systems. 

% RQ5: What is the percentage of research work carried out on the basis of single and double handed signs?
% (To identify the work done on single and double handed signs)
% we found that 48% of work on sign language
% recognition systems has been performed on single handed
% signs, followed by 20% on double handed signs and 32% on
% both single and double handed signs as shown in Fig. 21d.

% RQ6: What are the existing methodologies and techniques to recognize sign language recognition?
% (To identify and compare the existing methodologies and techniques used for sign language recognition)
% Fig. 21e depicts that the majority
% of the work on sign language recognition systems has been
% performed using NN (28%), followed by hybrid techniques
% (21%), SVM (20%), HMM (20%), while the minimum
% amount of work has been performed using DTW, KNN,
% CNN and other techniques. 

% RQ7: What is the accuracy and coverage of existing sign language recognition systems?
% (To identify the recognition rate of existing sign language recognition systems on the trained dataset)
% we observed that for all the sign language systems there are 66%
% of sign language recognition systems who achieved average
% accuracy of greater than 90%, while 23% of the systems have
% accuracy between 80 and 89%. There are only 11% systems
% whose accuracy is less than 80% as shown in Fig. 21f.



% ===================================================
% \cite{papastratis-2021-ai-technologies-sl}

% Abstract
% AI technologies can play an important role in breaking down the communication barriers
% of deaf or hearing-impaired people with other communities, contributing significantly to their social
% inclusion. Recent advances in both sensing technologies and AI algorithms have paved the way for
% the development of various applications aiming at fulfilling the needs of deaf and hearing-impaired
% communities. To this end, this survey aims to provide a comprehensive review of state-of-the-art
% methods in sign language capturing, recognition, translation and representation, pinpointing their
% advantages and limitations.

% Introduction
% The research community has long identified the
% need for developing sign language technologies to facilitate the communication and social
% inclusion of hearing-impaired people. Although the development of such technologies
% can be really challenging due to the existence of numerous sign languages and the lack of
% large annotated datasets, the recent advances in AI and machine learning have played a
% significant role towards automating and enhancing such technologies.

% Sign language recognition (SLR) involves the development of powerful machine learning
% algorithms to robustly classify human articulations to isolated signs or continuous sentences.
% Current limitations in SLR lie in the lack of large annotated datasets that greatly
% affect the accuracy and generalization ability of SLR methods, as well as the difficulty in
% identifying sign boundaries in continuous SLR scenarios.


% 3. Sign Language Capturing
% Sign language capturing involves the recording of sign gestures using appropriate
% sensor setups. The purpose is to capture discriminative information from the signs that will
% allow the study, recognition and 3D representation of signs at later stages. Moreover, sign
% language capturing enables the construction of large datasets that can be used to accurately
% train and evaluate machine learning sign language recognition and translation algorithms.

% 3.1. Capturing Sensors
% [...]
% Each of the aforementioned sensor setups for sign language capturing has different
% characteristics, which makes it suitable for different applications. Kinect sensors provide
% high resolution RGB and depth information but their accuracy is restricted by the distance
% from the sensors. Leap Motion also requires a small distance between the sensor and the
% subject, but their low computational requirements enable its usage in real-time applications.
% Multi-camera setups are capable of providing highly accurate results at the expense of
% increased complexity and computational requirements. A myo armband that can detect
% EMG and inertial signals is also used in few works but the inertial signals may be distorted
% by body motions when people are walking. Smartwatches are really popular nowadays
% and they can also be used for sign language capturing but their output can be quite noisy
% due to unexpected body movements. Finally, datagloves can provide highly accurate sign
% language capturing results in real-time. However, the tuning of its components (i.e., flex
% sensor, accelerometer, gyroscope) may require a trial and error process that is impractical
% and time-consuming. In addition, signers tend to not prefer datagloves for sign language
% capturing as they are considered invasive.

% 3.2. Datasets
% Datasets are crucial for the performance of methodologies regarding sign language
% recognition, translation and synthesis and as a result a lot of attention has been drawn
% towards the accurate capturing of signs and their meticulous annotation. The majority
% of the existing publicly available datasets are captured with visual sensors and are
% presented below.
% [...]
% A discussion about the aforementioned datasets can be made at this stage, while a
% detailed overview of the dataset characteristics is provided on Table 1. It can be seen
% that over time datasets become larger in size (i.e., number of samples) with more signers
% involved in them, as well as contain high resolution videos captured under various and
% challenging illumination and background conditions. Moreover, new datasets usually
% include different modalities (i.e., RGB, depth and skeleton). Recording sign language
% videos using many signers is very important, since each person performs signs with
% different speed, body posture and face expression. Moreover, high resolution videos
% capture more clearly small but important details, such as finger movements and face
% expressions, which are crucial cues for sign language understanding. Datasets with videos
% captured under different conditions enable deep networks to extract highly discriminative
% features for sign language classification. As a result, methodologies trained in such datasets
% can obtain greatly enhanced representation and generalization capabilities and achieve high
% recognition performances. Furthermore, although RGB information is the predominant
% modality used for sign language recognition, additional modalities, such as skeleton and
% depth information, can provide complementary information to the RGB modality and
% significantly improve the performance of SLR methods.


% 4. Sign Language Recognition
% Sign language recognition (SLR) is the task of recognizing sign language glosses from
% video streams. It is a very important research area since it can bridge the communication
% gap between hearing and Deaf people, facilitating the social inclusion of hearing-impaired
% people. Moreover, sign language recognition can be classified into isolated and continuous
% based on whether the video streams contain an isolated gloss or a gloss sequence that
% corresponds to a sentence.

% 4.1. Continuous Sign Language Recognition
% 4.2. Isolated Sign Language Recognition
% [...]
% 4.3. Sign Language Translation
% Sign Language Translation is the task of translating videos with sign language into
% spoken language by modeling not only the glosses but also the language structure and
% grammar. It is an important research area that facilitates the communication between the
% Deaf and other communities. Moreover, the SLT task is more challenging compared to
% CSLR due to the additional linguistic rules and the representation of spoken languages.
% SLT methods are usually evaluated using the bilingual evaluation understudy (BLEU)
% metric [92]. BLEU is a translation quality score that evaluates the correspondence between
% the predicted translation and the ground truth text.
% [...]

% 7. Conclusions and Future Directions
% The aim of this review is to familiarize researchers with sign language technologies and assist them
% towards developing better approaches.
% In the field of sign language capturing, it is essential to select an optimal sensor for
% capturing signs for a task that highly depends on various constraints (e.g., cost, speed,
% accuracy, etc.). For instance, wearable sensors (i.e., gloves) are expensive and capture only
% hand joints and arm movements, while in recognition applications, the user is required
% to use gloves. On the other hand, camera sensors, such as web or smartphone cameras,
% are inexpensive and capture the most substantial information, like the face and the body
% posture, which are crucial for sign language.
% Concerning CSLR approaches, most of the existing works adopt 2D CNNs with
% temporal convolutional networks or recurrent neural networks that use video as input.
% In general, 2D methods have lower training complexity compared to 3D architectures and
% produce better CSLR performance. Moreover, it is experimentally shown that multi-modal
% architectures that utilize optical flow or human pose information, achieve slightly higher
% recognition rates than unimodal methods. In addition, CSLR performance on datasets with
% large vocabularies of more than 1000 words, such as Phoenix-2014, or datasets with unseen
% words on the test sets, such as CSL Split 2 and GSL SD, is far from perfect. Furthermore,
% ISLR methods have been extensively explored and have achieved high recognition rates on
% large-scale datasets. However, they are not suitable for real-life applications since they are
% trained to detect and classify isolated signs on pre-segmented videos.
% Sign language translation methods have shown promising results although they are
% not exhaustively explored. The majority of the SLT methods adopt architectures from the
% field of neural machine translation and video captioning. These approaches are of great
% importance, since they translate sign language into spoken counterparts and can be used
% to facilitate the communication between the Deaf community and other groups. To this
% end, this research field requires additional attention from the research community.
% [...]
% From the chart in Figure 3a, it can be seen that most existing works deal with sign
% language recognition, while sign language capturing and translation methods are still not
% thoroughly explored. It is strongly believed that these research areas should be explored
% more in future works.




% ===================================================
% \cite{cooper-2011-slr}

% 1 Motivation
% While automatic speech recognition has now advanced to the point of being commercially
% available, automatic SLR is still in its infancy. Currently all commercial
% translation services are human based, and therefore expensive, due to the experienced
% personnel required.
% SLR aims to develop algorithms and methods to correctly identify a sequence
% of produced signs and to understand their meaning. Many approaches to SLR incorrectly
% treat the problem as Gesture Recognition (GR). So research has thus far
% focused on identifying optimal features and classification methods to correctly label
% a given sign from a set of possible signs. However, sign language is far more than
% just a collection of well specified gestures.
% Sign languages pose the challenge that they are multi-channel; conveying meaning
% through many modes at once. While the studies of sign language linguistics are
% still in their early stages, it is already apparent that this makes many of the techniques
% used by speech recognition unsuitable for SLR. In addition, publicly available
% data sets are limited both in quantity and quality, rendering many traditional
% computer vision learning algorithms inadequate for the task of building classifiers.
% However, even given the lack of translation tools, most public services are not translated
% into sign. There is no commonly-used, written form of sign language, so all
% written communication is in the local spoken language.

% [...]
% Although SLR and speech recognition are drastically different in many respects,
% they both suffer from similar issues; co-articulation between signs means that a sign
% will be modified by those either side of it. Inter-signer differences are large; every
% signer has their own style, in the same way that everyone has their own accent or
% handwriting. Also similar to handwriting, signers can be either left hand or right
% hand dominant. For a left handed signer, most signs will be mirrored, but time line
% specific ones will be kept consistent with the cultural ‘left to right’ axis. While it
% is not obvious how best to include these higher level linguistic constructs of the
% language, it is obviously essential if true, continuous SLR is to become reality.

% ---
% 3 Data Acquisition and Feature Extraction
% Acquiring data is the first step in a SLR system. Given that much of the meaning
% in sign language is conveyed through manual features, this has been the area of
% focus of the research up to the present as noted by Ong and Ranganath in their 2005
% survey [82].
% Many early SLR systems used data gloves and accelerometers to acquire specifics
% of the hands. The measurements (x,y,z, orientation, velocity etc) were measured directly
% using a sensor such as the Polhemus tracker [103] or DataGlove [54, 99].
% More often than not, the sensor input was of sufficient discriminatory power that feature
% extraction was bypassed and the measurements used directly as features [34].
% While these techniques gave the advantage of accurate positions, they did not allow
% full natural movement and constricted the mobility of the signer, altering the
% signs performed. Trials with a modified glove-like device, which was less constricting
% [43], attempted to address this problem. However, due to the the prohibitive
% costs of such approaches, the use of vision has become more popular. In the case of
% vision input, a sequence of images are captured from a combination of cameras (e.g.
% monocular [115], stereo [47], orthogonal [90]) or other non-invasive sensors. Segen
% and Kumar [87] used a camera and calibrated light source to compute depth, and
% Feris et al. [30] used a number of external light sources to illuminate a scene and
% then used multi-view geometry to construct a depth image. Starner et al. [91] used
% a front view camera in conjunction with a head mounted camera facing down on
% the subject’s hands to aid recognition. Depth can also be inferred using stereo cameras
% as was done by Munoz-Salinas et al. [72] or by using side/vertical mounted
% cameras as with Vogler and Metaxas [100] or the Boston ASL data set [75]. There
% are several projects which are creating sign language data sets; in Germany there is
% the DGS-Korpus dictionary project collecting data across the country over a 15yr
% period [22] or the similar project on a smaller scale in the UK by the BSL Corpus
% Project [14]. However, these data sets are directed at linguistic research, whereas
% the cross domain European project DictaSign [23] aims to produce a multi-lingual
% data set suitable for both linguists and computer vision scientists.

% 3.1 Manual Features
% Sign language involves many features which are based around the hands, in general
% there are hand shape/orientation (pose) and movement trajectories, which are similar
% in principle to gestures. A survey of GR was performed by Mitra and Acharya [70]
% giving an overview of the field as it stood in 2007. While many GR techniques are
% applicable, Sign language offers a more complex challenge than the traditionally
% more confined domain of gesture recognition.
% 3.1.1 Tracking Based
% 3.1.2 Non-Tracking Based
% 3.1.3 Hand Shape
% [...]

% 3.2 Finger Spelling
% Manual features are also extended to finger spelling, a subset of sign language.
% Recognising finger spelling requires careful description of the shapes of the hands
% and for some languages the motion of the hands.
% [...]

% 3.3 Non-Manual Features
% In addition to the manual features, there is a significant amount of information contained
% in the non-manual channels. The most notable of these are the facial expressions,
% lip shapes (as used by lip readers), as well as head pose which was recently
% surveyed by Murphy-Chutorian and Trivedi. [74] Little work has currently been
% performed on body pose, which plays a part during dialogues and stories.
% [...]

% ---
% 4 Recognition
% While some machine learning techniques were covered briefly in the section 3.1.3,
% this section focusses on how they have been applied to the task of sign recognition.
% The previous section looked at the low level features which provide the basis for
% SLR. In this section it is shown how machine learning can create combinations of
% these low level features to accurately describe a sign, or a subunit of sign

% 4.1 Classification Methods
% The earliest work on SLR applied NNs. However, given the success enjoyed by
% HMMs in the field of speech recognition, and the similarity of the problem of speech
% recognition and SLR, HMM based classification has dominated SLR since the mid
% 90’s.
% [...]

% 4.2 Phoneme Level Representations
% Work in the field of sign language linguistics has informed the features used for detection.
% This is clearly shown in work which classifies in two stages; using first a sign
% sub-unit layer, followed by a sign level layer. This offers SLR the same advantages
% as it offered speech recognition. Namely a scalable approach to large vocabularies
% as well as a more robust solution for time variations between examples.
% [...]

% ---
% 5 Research Frontiers
% There are many facets of SLR which have attracted attention in the computer vision
% community. This section serves to outline the areas which are currently generating
% the most interest due to the challenges they propose. While some of these are recent
% topics, others have been challenging computer vision experts for many years. Offered
% here is a brief overview of the seminal work and the current state of the art in
% each area.

% 5.1 Continuous Sign Recognition
% The majority of work on SLR has been focused on recognising isolated instances of
% signs, this is not applicable to a real world sign language recognition system. The
% task of recognising continuous sign language is complicated primarily by the problem
% that in natural sign language, the transition between signs is not clearly marked
% because the hands will be moving to the starting position of the next sign. This
% is referred to as the movement epenthesis or co-articulation (which borrows from
% speech terminology).
% [...]

% 5.2 Signer Independence
% A major problem relating to recognition is that of applying the system to a signer
% on whom the system has not been trained.
% [...]

% 5.3 Fusing Multi-Modal Sign Data
% From the review of SLR by Ong and Ranganath [82], one of their main observations
% is the lack of attention that non-manual features has received in the literature.
% This is still the case several years on. Much of the information in a sign is conveyed
% through this channel, and particularly there are signs that are identical in respect of
% the manual features and only distinguishable by the non-manual features accompanying
% the sign. The difficulty is identifying exactly which elements are important to
% the sign, and which elements are coincidental. For example, does the blink of the
% signer convey information valuable to the sign, or was the signer simply blinking?
% This problem of identifying the parts of the sign that contains information relevant to
% the understanding of the sign makes SLR a complex problem to solve.
% [...]

% 5.4 Using Linguistics
% The task of recognition is often simplified by forcing the possible word sequence
% to conform to a grammar which limits the potential choices and thereby improves
% recognition rates [91, 104, 12, 45].
% [...]

% 5.5 Generalising to More Complex Corpora
% Due to the lack of adequately labelled data sets, research has turned to weakly supervised
% approaches. Several groups have presented work aligning subtitles with
% signed TV broadcasts.
% [...]

% ---
% 6 Conclusions
% SLR has long since advanced beyond classifying isolated signs or alphabet forms
% for finger spelling. While the field may continue to draw on the advances in GR
% the focus has shifted to approach the more linguistic features associated with the
% challenge. Work has developed on extracting signs from continuous streams and
% using linguistic grammars to aid recognition. However, there is still much to be
% learnt from relevant fields such as speech recognition or hand writing recognition.
% In addition, while some have imposed grammatical rules from linguistics, others
% have looked at data driven approaches, both have their merits since the linguistics
% of most sign languages are still in their infancy.
% While the community continues to discuss the need for including non-manual
% features, few have actually done so. Those which have [2, 5], concentrate solely on
% the facial expressions of sign. There is still much to be explored in the veins of body
% posture or placement and classifier (hand shape) combinations.
% Finally, to compound all these challenges, there is the issue of signer independence.
% While larger data sets are starting to appear, few allow true tests of signer
% independence over long continuous sequences. Maybe this is one of the most urgent
% problems in SLR that of creating data sets which are not only realistic, but also well
% annotated to facilitate machine learning.
% Despite these problems recent uses of SLR include translation to spoken language,
% or to another sign language when combined with avatar technology [3, 25].