% APRENDIZAGEM PROFUNDA

\subsection{Aprendizagem profunda e modelos sequenciais}
\label{sec:am-ap}

Segundo \citeonline{goodfellow-2016-deep-learning,lecun-2015-deep-learning}, a \acrfull{dl} consiste num tipo de \acrlong{ml} baseado em \acrshortpl{ann} que adotam um número muito grande de camadas de processamento para extrair progressivamente representações de níveis mais elevados a partir dos dados.
A esse tipo particular de \acrshort{ann} atribui-se o nome de \acrfull{dnn}.

Por conta de sua estrutura robusta, essas redes têm sido capazes de lidar com problemas muito complexos ao longo das últimas décadas e produzir progressos extremamente promissores para problemas que resistiram por muito tempo às melhores tentativas de avanço pela comunidade de \acrshort{ia}, principalmente aqueles envolvendo linguagem -- e que são abordados pela área de \acrfull{nlp}.



\citeonline{jurafsky-2022-speech-lang-processing}, por sua vez, definem a linguagem como sendo um fenômeno inerentemente temporal. Os autores afirmam que ela pode ser compreendida como uma sequência de eventos que desdobram-se ao longo do tempo como um fluxo contínuo de dados.
Dessa forma, para que fosse possível abordar esse aspecto temporal e lidar com dados organizados de maneira sequencial, foram estabelecidas arquiteturas específicas de \acrshortpl{dnn} que atualmente são conhecidas como redes neurais sequenciais ou modelos sequenciais.
Dentre as mais populares dessas arquiteturas estão a \acrfull{rnn} (e suas extensões, como o \acrfull{lstm} e a \acrfull{gru}) e o \textit{Transformer}.

As \acrshortpl{rnn} baseiam-se no trabalho de \citeonline{rumelhart-1986-rnns} e consistem de redes neurais que contêm ciclos (ou recorrências) em suas conexões, os quais fazem com que o valor de suas unidades sejam direta ou indiretamente dependentes de suas próprias saídas anteriores.
De um modo geral, elas funcionam processando cada palavra da sequência e combinando ela com o contexto ou estado oculto anterior para tentar prever a próxima palavra da sequência. Esse contexto, por sua vez, é capaz de representar as informações de todas as palavras anteriores daquela sequência.

Contudo, \citeonline{lecun-2015-deep-learning,goodfellow-2016-deep-learning,graves-2012-rnns} ressaltam que essas redes apresentaram limitações em armazenar informações por um período muito longo de tempo, dentre as quais estão os problemas conhecidos de \textit{gradient vanishing} (ou desaparecimento do gradiente) e \textit{gradient exploding} (ou explosão do gradiente). Isso demandou com que extensões dessa arquitetura fossem desenvolvidas no decorrer dos anos com o intuito abordar melhor essas questões.

Uma das principais extensões é a \acrshort{lstm}, que foi introduzida por \citeonline{hochreiter-1997-lstm}. De acordo com \citeonline{graves-2012-rnns,jurafsky-2022-speech-lang-processing}, a principal inovação dessa rede é a capacidade de aprender a gerenciar o contexto de forma automática, decidindo quando informações são necessárias e quando podem ser removidas, sem necessitar que uma estratégia explícita seja codificada para isso. Ele utiliza uma camada específica para representar esse contexto e um conjunto de portas, as quais controlam o fluxo de informações para dentro e para fora de suas células.
Segundo \citeonline{goodfellow-2016-deep-learning,lecun-2015-deep-learning}, essas redes são extremamente bem-sucedidas em diferentes tipos de aplicações, como reconhecimento e geração de caligrafia, reconhecimento de fala, \acrfull{mt}, legendagem de imagens e análise sintática.


A \acrshort{gru} é também uma extensão muito popular das \acrshortpl{rnn} e foi criada por \citeonline{cho-2014-gru} com o intuito de simplificar o desenho das unidades internas do \acrshort{lstm}. 
De acordo com \citeonline{goodfellow-2016-deep-learning,ravanelli-2018-li-gru}, elas diferenciam-se apenas pela forma como controlam o fluxo de informações entre suas camadas: enquanto o \acrshort{lstm} utiliza três portas em suas células internas (\textit{update gate}, \textit{forget gate} e \textit{output gate}), o \acrshort{gru} propõe a adoção de apenas duas portas para isso (\textit{update gate} e \textit{reset gate}).


Um outro tipo de arquitetura bastante utilizada no \acrshort{nlp} é a \textit{Sequence-to-Sequence} (ou Sequência para Sequência), também conhecida como \textit{Encoder-Decoder} (ou Codificador Decodificador). Ela foi apresentada por \citeonline{cho-2014-encoder-decoder,sutskever-2014-seq-to-seq} e sua estrutura é composta por duas redes neurais: uma codificadora, que recebe uma sequência de entrada e gera uma representação contextualizada dela -- que seria o contexto; e uma decodificadora, que produz uma sequência de saída específica para a tarefa em questão, conforme ilustrado na \autoref{fig:encoder-decoder-arquitetura}. 
Esses redes codificadoras e decodificadoras são geralmente implementados utilizando-se \acrshortpl{rnn}, como o \acrshort{lstm} e o \acrshort{gru}. Além disso, algumas otimizações dessa arquitetura consideram a adição de uma camada de \textit{attention} (ou atenção) antes do decodificador com o objetivo de eliminar um gargalo observado ali por \citeonline{bahdanau-2015-mt-align-translate}.

\figura[p. 220]
{fig:encoder-decoder-arquitetura}
{capitulos/fundamentacao/imagens/encoder_decoder_arquitetura}
{width=0.90\textwidth}
{Arquitetura do \textit{Encoder-Decoder}: a sequência de entrada \(\{x_1, x_2, x_3, x_n\}\) é recebida pelo \textit{encoder} (à esquerda) e utilizada para gerar o contexto \(c\) (em verde), o qual é utilizado pelo \textit{decoder} (à direita) para produzir a sequência \(\{y_1, y_2, y_3, y_n\}\)}
{jurafsky-2022-speech-lang-processing}



O \textit{Transformer}, por sua vez, consiste num tipo de arquitetura que não é recorrente e, ao invés disso, baseia-se num mecanismo de \textit{attention} para estabelecer dependências globais entre os dados de entrada e saída.
Ele foi introduzido por \citeonline{vaswani-2017-transformer} e baseia-se na estrutura do \textit{Encoder-Decoder}, porém seus codificadores e decodificadores são compostos por blocos empilhados de redes multicamadas que combinam camadas lineares simples, redes \textit{feed-forward} e camadas de \textit{self-attention} (ou auto-atenção) -- as quais são a principal inovação aqui --, conforme ilustra a \autoref{fig:transformer-arquitetura}.


\figura[p. 3]
{fig:transformer-arquitetura}
{capitulos/fundamentacao/imagens/transformer_arquitetura}
{width=0.45\textwidth}
{Arquitetura do \textit{Transformer}: são utilizados blocos empilhados que combinam redes \textit{feed-forward} e camadas de \textit{self-attention} para o \textit{encoder} (à esquerda) e o \textit{decoder} (à direita); os \textit{embeddings} (abaixo) recebem uma codificação posicional para que seja considerada a ordem de suas sequências}
{vaswani-2017-transformer}


Segundo \citeonline{wolf-2020-transformers}, o \textit{Transformer} é escalável e capaz de capturar o contexto de sequências muito longas, e isso possibilitou a construção e aplicação de modelos de maior capacidade para uma grande variedade de tarefas. 
Devido a disso, ele tornou-se rapidamente a arquitetura dominante no \acrshort{nlp}, superando o desempenho de redes alternativas como as \acrshortpl{rnn} e a \acrfull{cnn} para tarefas  de compreensão e geração de linguagem natural.


Apesar disso, mesmo com as inovações introduzidas pelo \textit{Transformer}, é possível observar pela \autoref{fig:transformer-arquitetura} e \autoref{fig:encoder-decoder-arquitetura} que sua estrutura ainda preserva muitas semelhanças com o \textit{Encoder-Decoder}, da qual origina-se.
Em ambas estruturas, percebem-se dois grandes blocos interconectados, que interagem para codificar uma representação de contexto a partir de uma sequência de dados de entrada, e para decodificá-lo gerando uma nova sequência de saída. 
Essas sequências, por sua vez, podem possuir tamanhos distintos e permitem com que sejam modelados diferentes tipos de representações de linguagem como, por exemplo, uma sequência de texto que é traduzida de um idioma para outro, uma representação de fala que é transformada em texto (ou vice-versa), ou ainda uma resposta que é gerada para uma pergunta fornecida como entrada.

Devido a essa afinidade para lidar com tarefas de linguagem, ambas arquiteturas \textit{Transformer} e \textit{Encoder-Decoder} serão consideradas para avaliação da abordagem introduzida mais adiante neste trabalho.
